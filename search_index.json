[
["index.html", "Machine Learning with R Chapter 1 Prerequisites", " Machine Learning with R François de Ryckel 2017-02-13 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). For now, you have to install the development versions of bookdown from Github: devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need to install XeLaTeX. "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 4. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2016) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["literature.html", "Chapter 3 Literature", " Chapter 3 Literature Here is a review of existing methods. "],
["methods.html", "Chapter 4 Methods", " Chapter 4 Methods We describe our methods in this chapter. "],
["case-study-mushrooms-classification.html", "Chapter 5 Case Study - Mushrooms Classification 5.1 Import the data 5.2 Tidy the data 5.3 Understand the data 5.4 Communication 5.5 Example one 5.6 Example two", " Chapter 5 Case Study - Mushrooms Classification This chapter demonstrates how to classify muhsrooms as edible or not. It also answera the question: what are the main characteristics of an edible mushroom? This blog post gave us first the idea and we followed most of it. We also noticed that Kaggle has put online the same data set and classification exercise. We have taken inspiration from some posts here and here The data set is available on the Machine Learning Repository of the UC Irvine website. 5.1 Import the data The data set is given to us in a rough form and quite a bit of editing is necessary. # Load the data - we downloaded the data from the website and saved it into a .csv file library(readr) library(dplyr) mushroom &lt;- read_csv(&quot;dataset/Mushroom.csv&quot;, col_names = FALSE) glimpse(mushroom) ## Observations: 8,124 ## Variables: 23 ## $ X1 &lt;chr&gt; &quot;p&quot;, &quot;e&quot;, &quot;e&quot;, &quot;p&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;p&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;... ## $ X2 &lt;chr&gt; &quot;x&quot;, &quot;x&quot;, &quot;b&quot;, &quot;x&quot;, &quot;x&quot;, &quot;x&quot;, &quot;b&quot;, &quot;b&quot;, &quot;x&quot;, &quot;b&quot;, &quot;x&quot;, &quot;x&quot;... ## $ X3 &lt;chr&gt; &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;y&quot;, &quot;s&quot;, &quot;y&quot;, &quot;s&quot;, &quot;y&quot;, &quot;y&quot;, &quot;s&quot;, &quot;y&quot;, &quot;y&quot;... ## $ X4 &lt;chr&gt; &quot;n&quot;, &quot;y&quot;, &quot;w&quot;, &quot;w&quot;, &quot;g&quot;, &quot;y&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;y&quot;, &quot;y&quot;, &quot;y&quot;... ## $ X5 &lt;chr&gt; &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;f&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;... ## $ X6 &lt;chr&gt; &quot;p&quot;, &quot;a&quot;, &quot;l&quot;, &quot;p&quot;, &quot;n&quot;, &quot;a&quot;, &quot;a&quot;, &quot;l&quot;, &quot;p&quot;, &quot;a&quot;, &quot;l&quot;, &quot;a&quot;... ## $ X7 &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;... ## $ X8 &lt;chr&gt; &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;w&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;... ## $ X9 &lt;chr&gt; &quot;n&quot;, &quot;b&quot;, &quot;b&quot;, &quot;n&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;n&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;... ## $ X10 &lt;chr&gt; &quot;k&quot;, &quot;k&quot;, &quot;n&quot;, &quot;n&quot;, &quot;k&quot;, &quot;n&quot;, &quot;g&quot;, &quot;n&quot;, &quot;p&quot;, &quot;g&quot;, &quot;g&quot;, &quot;n&quot;... ## $ X11 &lt;chr&gt; &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;t&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;... ## $ X12 &lt;chr&gt; &quot;e&quot;, &quot;c&quot;, &quot;c&quot;, &quot;e&quot;, &quot;e&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;e&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;... ## $ X13 &lt;chr&gt; &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;... ## $ X14 &lt;chr&gt; &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;... ## $ X15 &lt;chr&gt; &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;... ## $ X16 &lt;chr&gt; &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;... ## $ X17 &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;... ## $ X18 &lt;chr&gt; &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;... ## $ X19 &lt;chr&gt; &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;... ## $ X20 &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;e&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;... ## $ X21 &lt;chr&gt; &quot;k&quot;, &quot;n&quot;, &quot;n&quot;, &quot;k&quot;, &quot;n&quot;, &quot;k&quot;, &quot;k&quot;, &quot;n&quot;, &quot;k&quot;, &quot;k&quot;, &quot;n&quot;, &quot;k&quot;... ## $ X22 &lt;chr&gt; &quot;s&quot;, &quot;n&quot;, &quot;n&quot;, &quot;s&quot;, &quot;a&quot;, &quot;n&quot;, &quot;n&quot;, &quot;s&quot;, &quot;v&quot;, &quot;s&quot;, &quot;n&quot;, &quot;s&quot;... ## $ X23 &lt;chr&gt; &quot;u&quot;, &quot;g&quot;, &quot;m&quot;, &quot;u&quot;, &quot;g&quot;, &quot;g&quot;, &quot;m&quot;, &quot;m&quot;, &quot;g&quot;, &quot;m&quot;, &quot;g&quot;, &quot;m&quot;... Basically we have 8124 mushrooms in the dataset. And each observation consists of 23 variables. As it stands, the data frame doesn’t look very meaningfull. We have to go back to the source to bring meaning to each of the variables and to the various levels of the categorical variables. 5.2 Tidy the data This is the least fun part of the workflow. We’ll start by giving names to each of the variables, then we specify the category for each variable. It is not necessary to do so but it does add meaning to what we do. # Rename the variables colnames(mushroom) &lt;- c(&quot;edibility&quot;, &quot;cap_shape&quot;, &quot;cap_surface&quot;, &quot;cap_color&quot;, &quot;bruises&quot;, &quot;odor&quot;, &quot;gill_attachement&quot;, &quot;gill_spacing&quot;, &quot;gill_size&quot;, &quot;gill_color&quot;, &quot;stalk_shape&quot;, &quot;stalk_root&quot;, &quot;stalk_surface_above_ring&quot;, &quot;stalk_surface_below_ring&quot;, &quot;stalk_color_above_ring&quot;, &quot;stalk_color_below_ring&quot;, &quot;veil_type&quot;, &quot;veil_color&quot;, &quot;ring_number&quot;, &quot;ring_type&quot;, &quot;spore_print_color&quot;, &quot;population&quot;, &quot;habitat&quot;) # Defining the levels for the categorical variables ## We make each variable as a factor library(purrr) mushroom &lt;- mushroom %&gt;% map_df(function(.x) as.factor(.x)) ## We redefine each of the category for each of the variables levels(mushroom$edibility) &lt;- c(&quot;edible&quot;, &quot;poisonous&quot;) levels(mushroom$cap_shape) &lt;- c(&quot;bell&quot;, &quot;conical&quot;, &quot;flat&quot;, &quot;knobbed&quot;, &quot;sunken&quot;, &quot;convex&quot;) levels(mushroom$cap_color) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$cap_surface) &lt;- c(&quot;fibrous&quot;, &quot;grooves&quot;, &quot;scaly&quot;, &quot;smooth&quot;) levels(mushroom$bruises) &lt;- c(&quot;no&quot;, &quot;yes&quot;) levels(mushroom$odor) &lt;- c(&quot;almond&quot;, &quot;creosote&quot;, &quot;foul&quot;, &quot;anise&quot;, &quot;musty&quot;, &quot;none&quot;, &quot;pungent&quot;, &quot;spicy&quot;, &quot;fishy&quot;) levels(mushroom$gill_attachement) &lt;- c(&quot;attached&quot;, &quot;free&quot;) levels(mushroom$gill_spacing) &lt;- c(&quot;close&quot;, &quot;crowded&quot;) levels(mushroom$gill_size) &lt;- c(&quot;broad&quot;, &quot;narrow&quot;) levels(mushroom$gill_color) &lt;- c(&quot;buff&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;chocolate&quot;, &quot;black&quot;, &quot;brown&quot;, &quot;orange&quot;, &quot;pink&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$stalk_shape) &lt;- c(&quot;enlarging&quot;, &quot;tapering&quot;) levels(mushroom$stalk_root) &lt;- c(&quot;missing&quot;, &quot;bulbous&quot;, &quot;club&quot;, &quot;equal&quot;, &quot;rooted&quot;) levels(mushroom$stalk_surface_above_ring) &lt;- c(&quot;fibrous&quot;, &quot;silky&quot;, &quot;smooth&quot;, &quot;scaly&quot;) levels(mushroom$stalk_surface_below_ring) &lt;- c(&quot;fibrous&quot;, &quot;silky&quot;, &quot;smooth&quot;, &quot;scaly&quot;) levels(mushroom$stalk_color_above_ring) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$stalk_color_below_ring) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$veil_type) &lt;- &quot;partial&quot; levels(mushroom$veil_color) &lt;- c(&quot;brown&quot;, &quot;orange&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$ring_number) &lt;- c(&quot;none&quot;, &quot;one&quot;, &quot;two&quot;) levels(mushroom$ring_type) &lt;- c(&quot;evanescent&quot;, &quot;flaring&quot;, &quot;large&quot;, &quot;none&quot;, &quot;pendant&quot;) levels(mushroom$spore_print_color) &lt;- c(&quot;buff&quot;, &quot;chocolate&quot;, &quot;black&quot;, &quot;brown&quot;, &quot;orange&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$population) &lt;- c(&quot;abundant&quot;, &quot;clustered&quot;, &quot;numerous&quot;, &quot;scattered&quot;, &quot;several&quot;, &quot;solitary&quot;) levels(mushroom$habitat) &lt;- c(&quot;wood&quot;, &quot;grasses&quot;, &quot;leaves&quot;, &quot;meadows&quot;, &quot;paths&quot;, &quot;urban&quot;, &quot;waste&quot;) glimpse(mushroom) ## Observations: 8,124 ## Variables: 23 ## $ edibility &lt;fctr&gt; poisonous, edible, edible, poisonous... ## $ cap_shape &lt;fctr&gt; convex, convex, bell, convex, convex... ## $ cap_surface &lt;fctr&gt; scaly, scaly, scaly, smooth, scaly, ... ## $ cap_color &lt;fctr&gt; brown, yellow, white, white, gray, y... ## $ bruises &lt;fctr&gt; yes, yes, yes, yes, no, yes, yes, ye... ## $ odor &lt;fctr&gt; pungent, almond, anise, pungent, non... ## $ gill_attachement &lt;fctr&gt; free, free, free, free, free, free, ... ## $ gill_spacing &lt;fctr&gt; close, close, close, close, crowded,... ## $ gill_size &lt;fctr&gt; narrow, broad, broad, narrow, broad,... ## $ gill_color &lt;fctr&gt; black, black, brown, brown, black, b... ## $ stalk_shape &lt;fctr&gt; enlarging, enlarging, enlarging, enl... ## $ stalk_root &lt;fctr&gt; equal, club, club, equal, equal, clu... ## $ stalk_surface_above_ring &lt;fctr&gt; smooth, smooth, smooth, smooth, smoo... ## $ stalk_surface_below_ring &lt;fctr&gt; smooth, smooth, smooth, smooth, smoo... ## $ stalk_color_above_ring &lt;fctr&gt; purple, purple, purple, purple, purp... ## $ stalk_color_below_ring &lt;fctr&gt; purple, purple, purple, purple, purp... ## $ veil_type &lt;fctr&gt; partial, partial, partial, partial, ... ## $ veil_color &lt;fctr&gt; white, white, white, white, white, w... ## $ ring_number &lt;fctr&gt; one, one, one, one, one, one, one, o... ## $ ring_type &lt;fctr&gt; pendant, pendant, pendant, pendant, ... ## $ spore_print_color &lt;fctr&gt; black, brown, brown, black, brown, b... ## $ population &lt;fctr&gt; scattered, numerous, numerous, scatt... ## $ habitat &lt;fctr&gt; urban, grasses, meadows, urban, gras... As each variables is categorical, let’s see how many categories are we speaking about? library(tibble) number_class &lt;- function(x){ x &lt;- length(levels(x)) } x &lt;- mushroom %&gt;% map_dbl(function(.x) number_class(.x)) %&gt;% as_tibble() %&gt;% rownames_to_column() %&gt;% arrange(desc(value)) colnames(x) &lt;- c(&quot;Variable name&quot;, &quot;Number of levels&quot;) print(x) ## # A tibble: 23 × 2 ## `Variable name` `Number of levels` ## &lt;chr&gt; &lt;dbl&gt; ## 1 gill_color 12 ## 2 cap_color 10 ## 3 stalk_color_above_ring 10 ## 4 stalk_color_below_ring 10 ## 5 odor 9 ## 6 spore_print_color 9 ## 7 habitat 7 ## 8 cap_shape 6 ## 9 population 6 ## 10 stalk_root 5 ## # ... with 13 more rows 5.3 Understand the data This is the circular phase of our dealing with data. This is where each of the transforming, visualizing and modeling stage reinforce each other to create a better understanding. 5.3.1 A. Transform the data We noticed from the previous section an issue with the veil_type variable. It has only one factor. So basically, it does not bring any information. Furthermore, factor variable with only one level do create issues later on at the modeling stage. R will throw out an error for the categorical variable that has only one level. So let’s take away that column. mushroom &lt;- mushroom %&gt;% select(- veil_type) Do we have any missing data? Most ML algorithms won’t work if we have missing data. map_dbl(mushroom, function(.x) {sum(is.na(.x))}) ## edibility cap_shape cap_surface ## 0 0 0 ## cap_color bruises odor ## 0 0 0 ## gill_attachement gill_spacing gill_size ## 0 0 0 ## gill_color stalk_shape stalk_root ## 0 0 0 ## stalk_surface_above_ring stalk_surface_below_ring stalk_color_above_ring ## 0 0 0 ## stalk_color_below_ring veil_color ring_number ## 0 0 0 ## ring_type spore_print_color population ## 0 0 0 ## habitat ## 0 Lucky us! We have no missing data. 5.3.2 A. Visualize the data This is one of the most important step in the DS process. This stage can gives us unexpected insights and often allows us to ask the right questions. library(ggplot2) ggplot(mushroom, aes(x = cap_surface, y = cap_color, col = edibility)) + geom_jitter(alpha = 0.5) + scale_color_manual(breaks = c(&quot;edible&quot;, &quot;poisonous&quot;), values = c(&quot;green&quot;, &quot;red&quot;)) If we want to stay safe, better bet on fibrous surface. Stay especially away from smooth surface, except if they are purple or green. ggplot(mushroom, aes(x = cap_shape, y = cap_color, col = edibility)) + geom_jitter(alpha = 0.5) + scale_color_manual(breaks = c(&quot;edible&quot;, &quot;poisonous&quot;), values = c(&quot;green&quot;, &quot;red&quot;)) Again, in case one don’t know about mushroom, it is better to stay away from all shapes except maybe for bell shape mushrooms. ggplot(mushroom, aes(x = gill_color, y = cap_color, col = edibility)) + geom_jitter(alpha = 0.5) + scale_color_manual(breaks = c(&quot;edible&quot;, &quot;poisonous&quot;), values = c(&quot;green&quot;, &quot;red&quot;)) ggplot(mushroom, aes(x = edibility, y = odor, col = edibility)) + geom_jitter(alpha = 0.5) + scale_color_manual(breaks = c(&quot;edible&quot;, &quot;poisonous&quot;), values = c(&quot;green&quot;, &quot;red&quot;)) Odor is defintely quite an informative predictor. Basically, if it smells fishy, spicy or pungent just stay away. If it smells like anise or almond you can go ahead. If it doesn’t smell anything, you have better chance that it is edible than not. TO DO: put a comment on what we see TO DO: put a mosaic graph 5.3.3 A. Modeling At this stage, we should have gathered enough information and insights on our data to choose appropriate modeling techniques. Before we go ahead, we need to split the data into a training and testing set set.seed(1810) mushsample &lt;- caret::createDataPartition(y = mushroom$edibility, times = 1, p = 0.8, list = FALSE) train_mushroom &lt;- mushroom[mushsample, ] test_mushroom &lt;- mushroom[-mushsample, ] We can check the quality of the splits in regards to our predicted (dependent) variable. round(prop.table(table(mushroom$edibility)), 2) ## ## edible poisonous ## 0.52 0.48 round(prop.table(table(train_mushroom$edibility)), 2) ## ## edible poisonous ## 0.52 0.48 round(prop.table(table(test_mushroom$edibility)), 2) ## ## edible poisonous ## 0.52 0.48 It seems like we have the right splits. 5.3.3.1 A. Use of Regression Tree As we have many categorical variables, regression tree is an ideal classification tools for such situation. We’ll use the rpart package. Let’s give it a try without any customization. library(rpart) library(rpart.plot) set.seed(1810) model_tree &lt;- rpart(edibility ~ ., data = train_mushroom, method = &quot;class&quot;) model_tree ## n= 6500 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 6500 3133 edible (0.51800000 0.48200000) ## 2) odor=almond,anise,none 3468 101 edible (0.97087659 0.02912341) ## 4) spore_print_color=buff,chocolate,black,brown,orange,purple,white,yellow 3408 41 edible (0.98796948 0.01203052) * ## 5) spore_print_color=green 60 0 poisonous (0.00000000 1.00000000) * ## 3) odor=creosote,foul,musty,pungent,spicy,fishy 3032 0 poisonous (0.00000000 1.00000000) * caret::confusionMatrix(data=predict(model_tree, type = &quot;class&quot;), reference = train_mushroom$edibility, positive=&quot;edible&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction edible poisonous ## edible 3367 41 ## poisonous 0 3092 ## ## Accuracy : 0.9937 ## 95% CI : (0.9915, 0.9955) ## No Information Rate : 0.518 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9874 ## Mcnemar&#39;s Test P-Value : 4.185e-10 ## ## Sensitivity : 1.0000 ## Specificity : 0.9869 ## Pos Pred Value : 0.9880 ## Neg Pred Value : 1.0000 ## Prevalence : 0.5180 ## Detection Rate : 0.5180 ## Detection Prevalence : 0.5243 ## Balanced Accuracy : 0.9935 ## ## &#39;Positive&#39; Class : edible ## We have quite an issue here. 40 mushrooms have been predicted as edible but were actually poisonous. That should not be happening. So we’ll set up a penalty for wrongly predicting a mushroom as edible when in reality it is poisonous. A mistake the other way is not as bad. At worst we miss on a good recipe! So let’s redo our tree with a penalty for wrongly predicting poisonous. To do this, we introduce a penalty matrix that we’ll use as a parameter in our rpart function. penalty_matrix &lt;- matrix(c(0, 1, 10, 0), byrow = TRUE, nrow = 2) model_tree_penalty &lt;- rpart(edibility ~ ., data = train_mushroom, method = &quot;class&quot;, parms = list(loss = penalty_matrix)) caret::confusionMatrix(data=predict(model_tree_penalty, type = &quot;class&quot;), reference = train_mushroom$edibility, positive=&quot;edible&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction edible poisonous ## edible 3367 0 ## poisonous 0 3133 ## ## Accuracy : 1 ## 95% CI : (0.9994, 1) ## No Information Rate : 0.518 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.000 ## Specificity : 1.000 ## Pos Pred Value : 1.000 ## Neg Pred Value : 1.000 ## Prevalence : 0.518 ## Detection Rate : 0.518 ## Detection Prevalence : 0.518 ## Balanced Accuracy : 1.000 ## ## &#39;Positive&#39; Class : edible ## So introducing a penalty did the job; it gave us a perfect prediction and saves us from a jounrey at the hospital. Another way to increase the accuracy of our tree model is to play on the cp parameter. We start to build a tree with a very low cp (that is we’ll have a deep tree). The idea is that we then prune it later. model_tree &lt;- rpart(edibility ~ ., data = train_mushroom, method = &quot;class&quot;, cp = 0.00001) To prune a tree, we first have to find the cp that gives the lowest xerror or cross-validation error. We can find the lowest xerror using either the printcp or plotcp function. printcp(model_tree) ## ## Classification tree: ## rpart(formula = edibility ~ ., data = train_mushroom, method = &quot;class&quot;, ## cp = 1e-05) ## ## Variables actually used in tree construction: ## [1] cap_surface habitat odor ## [4] spore_print_color stalk_color_below_ring stalk_root ## ## Root node error: 3133/6500 = 0.482 ## ## n= 6500 ## ## CP nsplit rel error xerror xstd ## 1 0.9677625 0 1.0000000 1.0000000 0.01285833 ## 2 0.0191510 1 0.0322375 0.0322375 0.00318273 ## 3 0.0063837 2 0.0130865 0.0130865 0.00203731 ## 4 0.0022343 3 0.0067028 0.0067028 0.00146032 ## 5 0.0011171 5 0.0022343 0.0022343 0.00084402 ## 6 0.0000100 7 0.0000000 0.0022343 0.00084402 We can see here that that the lowest xerror happen at the 5th split. plotcp(model_tree) model_tree$cptable[which.min(model_tree$cptable[, &quot;xerror&quot;]), &quot;CP&quot;] ## [1] 0.00111714 So now we can start pruning our tree with the cp that gives the lowest cross-validation error. bestcp &lt;- round(model_tree$cptable[which.min(model_tree$cptable[, &quot;xerror&quot;]), &quot;CP&quot;], 4) model_tree_pruned &lt;- prune(model_tree, cp = bestcp) Let’s have a quick look at the tree as it stands rpart.plot(model_tree_pruned, extra = 104, box.palette = &quot;GnBu&quot;, branch.lty = 3, shadow.col = &quot;gray&quot;, nn = TRUE) How does the model perform on the train data? #table(train_mushroom$edibility, predict(model_tree, type=&quot;class&quot;)) caret::confusionMatrix(data=predict(model_tree_pruned, type = &quot;class&quot;), reference = train_mushroom$edibility, positive=&quot;edible&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction edible poisonous ## edible 3367 0 ## poisonous 0 3133 ## ## Accuracy : 1 ## 95% CI : (0.9994, 1) ## No Information Rate : 0.518 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.000 ## Specificity : 1.000 ## Pos Pred Value : 1.000 ## Neg Pred Value : 1.000 ## Prevalence : 0.518 ## Detection Rate : 0.518 ## Detection Prevalence : 0.518 ## Balanced Accuracy : 1.000 ## ## &#39;Positive&#39; Class : edible ## It seems like we have a perfect accuracy on our training set. It is quite rare to have such perfect accuracy. Let’s check how it fares on the testing set. test_tree &lt;- predict(model_tree, newdata = test_mushroom) caret::confusionMatrix(data = predict(model_tree, newdata = test_mushroom, type = &quot;class&quot;), reference = test_mushroom$edibility, positive = &quot;edible&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction edible poisonous ## edible 841 0 ## poisonous 0 783 ## ## Accuracy : 1 ## 95% CI : (0.9977, 1) ## No Information Rate : 0.5179 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.0000 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 1.0000 ## Prevalence : 0.5179 ## Detection Rate : 0.5179 ## Detection Prevalence : 0.5179 ## Balanced Accuracy : 1.0000 ## ## &#39;Positive&#39; Class : edible ## Perfect prediction here as well. 5.3.3.2 A. Use of Random Forest We usually use random forest if a tree is not enough. In this case, as we have perfect prediction using a single tree, it is not really necessary to use a Random Forest algorithm. We just use for learning sake without tuning any of the parameters. library(randomForest) model_rf &lt;- randomForest(edibility ~ ., ntree = 50, data = train_mushroom) plot(model_rf) The default number of trees for the random forest is 500; we just use 50 here. As we can see on the plot, above 20 trees, the error isn’t decreasing anymore. And actually, the error seems to be 0 or almost 0. The next step can tell us this more accurately. print(model_rf) ## ## Call: ## randomForest(formula = edibility ~ ., data = train_mushroom, ntree = 50) ## Type of random forest: classification ## Number of trees: 50 ## No. of variables tried at each split: 4 ## ## OOB estimate of error rate: 0% ## Confusion matrix: ## edible poisonous class.error ## edible 3367 0 0 ## poisonous 0 3133 0 Altough it is not really necessary to this here as we have a perfect prediction, we can use the confusionMatrix function from the caret pacakge. caret::confusionMatrix(data = model_rf$predicted, reference = train_mushroom$edibility , positive = &quot;edible&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction edible poisonous ## edible 3367 0 ## poisonous 0 3133 ## ## Accuracy : 1 ## 95% CI : (0.9994, 1) ## No Information Rate : 0.518 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.000 ## Specificity : 1.000 ## Pos Pred Value : 1.000 ## Neg Pred Value : 1.000 ## Prevalence : 0.518 ## Detection Rate : 0.518 ## Detection Prevalence : 0.518 ## Balanced Accuracy : 1.000 ## ## &#39;Positive&#39; Class : edible ## If we want to look at the most important variable in terms of predicting edibility in our model, we can do that using the Mean Decreasing Gini varImpPlot(model_rf, sort = TRUE, n.var = 10, main = &quot;The 10 variables with the most predictive power&quot;) Another way to look at the predictible power of the variables is to use the importance extractor function. library(tibble) importance(model_rf) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Variable&quot;) %&gt;% arrange(desc(MeanDecreaseGini)) %&gt;% head(10) ## Variable MeanDecreaseGini ## 1 odor 1115.85522 ## 2 spore_print_color 477.71557 ## 3 gill_color 319.02467 ## 4 stalk_surface_above_ring 235.59574 ## 5 gill_size 194.56155 ## 6 stalk_surface_below_ring 172.26749 ## 7 stalk_root 132.26045 ## 8 ring_type 129.88445 ## 9 population 79.42030 ## 10 gill_spacing 63.42436 We could compare that with the important variables from the classification tree obtained above. model_tree_penalty$variable.importance %&gt;% as_tibble() %&gt;% rownames_to_column(var = &quot;variable&quot;) %&gt;% arrange(desc(value)) %&gt;% head(10) ## # A tibble: 10 × 2 ## variable value ## &lt;chr&gt; &lt;dbl&gt; ## 1 odor 848.00494 ## 2 spore_print_color 804.39831 ## 3 gill_color 503.71270 ## 4 stalk_surface_above_ring 501.28385 ## 5 stalk_surface_below_ring 453.92877 ## 6 ring_type 450.29286 ## 7 ring_number 170.56141 ## 8 stalk_root 117.78800 ## 9 habitat 98.22176 ## 10 stalk_color_below_ring 74.72602 Interestingly gill_size which is the 5th most important predictor in the random forest does not appear in the top 10 of our classification tree. Now we apply our model to our testing set. test_rf &lt;- predict(model_rf, newdata = test_mushroom) # Quick check on our prediction table(test_rf, test_mushroom$edibility) ## ## test_rf edible poisonous ## edible 841 0 ## poisonous 0 783 Perfect Prediction! 5.3.3.3 A. Use of SVM library(e1071) model_svm &lt;- svm(edibility ~. , data=train_mushroom, cost = 1000, gamma = 0.01) Check the prediction test_svm &lt;- predict(model_svm, newdata = test_mushroom) table(test_svm, test_mushroom$edibility) ## ## test_svm edible poisonous ## edible 841 0 ## poisonous 0 783 And perfect prediction again! 5.4 Communication With some fine tuning, a regression tree managed to predict accurately the edibility of mushroom. They were 2 parameters to look at: the cpand the penalty matrix. Random Forest and SVM achieved similar results out of the box. The regression tree approach has to be prefered as it is a lot easier to grasp the results from a tree than from a SVM algorithm. For sure I will take my little tree picture next time I go shrooming. That said, I will still only go with a good mycologist. We would love to hear from you. Give us feedback below. 5.5 Example one 5.6 Example two "],
["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],
["references.html", "References", " References "]
]
