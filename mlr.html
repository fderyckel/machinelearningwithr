<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning with R</title>
  <meta name="description" content="This book is about using R for machine learning purposes.">
  <meta name="generator" content="bookdown 0.5.4 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book is about using R for machine learning purposes." />
  <meta name="github-repo" content="fderyckel/machinelearningwithr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning with R" />
  
  <meta name="twitter:description" content="This book is about using R for machine learning purposes." />
  

<meta name="author" content="FranÃ§ois de Ryckel">


<meta name="date" content="2017-12-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="testinference.html">
<link rel="next" href="logistic.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Machine Learning with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#pre-requisite-and-conventions"><i class="fa fa-check"></i><b>1.1</b> Pre-requisite and conventions</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#organization"><i class="fa fa-check"></i><b>1.2</b> Organization</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="testinference.html"><a href="testinference.html"><i class="fa fa-check"></i><b>2</b> Tests and inferences</a><ul>
<li class="chapter" data-level="2.1" data-path="testinference.html"><a href="testinference.html#normality"><i class="fa fa-check"></i><b>2.1</b> Assumption of normality</a><ul>
<li class="chapter" data-level="2.1.1" data-path="testinference.html"><a href="testinference.html#visual-check-of-normality"><i class="fa fa-check"></i><b>2.1.1</b> Visual check of normality</a></li>
<li class="chapter" data-level="2.1.2" data-path="testinference.html"><a href="testinference.html#normality-tests"><i class="fa fa-check"></i><b>2.1.2</b> Normality tests</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="testinference.html"><a href="testinference.html#ttest"><i class="fa fa-check"></i><b>2.2</b> T-tests</a></li>
<li class="chapter" data-level="2.3" data-path="testinference.html"><a href="testinference.html#anova---analyse-of-variance."><i class="fa fa-check"></i><b>2.3</b> ANOVA - Analyse of variance.</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mlr.html"><a href="mlr.html"><i class="fa fa-check"></i><b>3</b> Single &amp; Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="mlr.html"><a href="mlr.html#single-variable-regression"><i class="fa fa-check"></i><b>3.1</b> Single variable regression</a></li>
<li class="chapter" data-level="3.2" data-path="mlr.html"><a href="mlr.html#multi-variables-regression"><i class="fa fa-check"></i><b>3.2</b> Multi-variables regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="mlr.html"><a href="mlr.html#predicting-wine-price-again"><i class="fa fa-check"></i><b>3.2.1</b> Predicting wine price (again!)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="mlr.html"><a href="mlr.html#model-diagnostic-and-evaluation"><i class="fa fa-check"></i><b>3.3</b> Model diagnostic and evaluation</a></li>
<li class="chapter" data-level="3.4" data-path="mlr.html"><a href="mlr.html#final-example---boston-dataset---with-backward-elimination"><i class="fa fa-check"></i><b>3.4</b> Final example - Boston dataset - with backward elimination</a><ul>
<li class="chapter" data-level="3.4.1" data-path="mlr.html"><a href="mlr.html#model-diagmostic"><i class="fa fa-check"></i><b>3.4.1</b> Model diagmostic</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="mlr.html"><a href="mlr.html#references"><i class="fa fa-check"></i><b>3.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="logistic.html"><a href="logistic.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html#the-logistic-equation."><i class="fa fa-check"></i><b>4.2</b> The logistic equation.</a></li>
<li class="chapter" data-level="4.3" data-path="logistic.html"><a href="logistic.html#performance-of-logistic-regression-model"><i class="fa fa-check"></i><b>4.3</b> Performance of Logistic Regression Model</a></li>
<li class="chapter" data-level="4.4" data-path="logistic.html"><a href="logistic.html#setting-up"><i class="fa fa-check"></i><b>4.4</b> Setting up</a></li>
<li class="chapter" data-level="4.5" data-path="logistic.html"><a href="logistic.html#example-1---graduate-admission"><i class="fa fa-check"></i><b>4.5</b> Example 1 - Graduate Admission</a></li>
<li class="chapter" data-level="4.6" data-path="logistic.html"><a href="logistic.html#example-2---diabetes"><i class="fa fa-check"></i><b>4.6</b> Example 2 - Diabetes</a><ul>
<li class="chapter" data-level="4.6.1" data-path="logistic.html"><a href="logistic.html#accounting-for-missing-values"><i class="fa fa-check"></i><b>4.6.1</b> Accounting for missing values</a></li>
<li class="chapter" data-level="4.6.2" data-path="logistic.html"><a href="logistic.html#imputting-missing-values"><i class="fa fa-check"></i><b>4.6.2</b> Imputting Missing Values</a></li>
<li class="chapter" data-level="4.6.3" data-path="logistic.html"><a href="logistic.html#roc-and-auc"><i class="fa fa-check"></i><b>4.6.3</b> ROC and AUC</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="logistic.html"><a href="logistic.html#references-1"><i class="fa fa-check"></i><b>4.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="softmax-and-multinomial-regressions.html"><a href="softmax-and-multinomial-regressions.html"><i class="fa fa-check"></i><b>5</b> Softmax and multinomial regressions</a><ul>
<li class="chapter" data-level="5.1" data-path="softmax-and-multinomial-regressions.html"><a href="softmax-and-multinomial-regressions.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>5.1</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="5.2" data-path="softmax-and-multinomial-regressions.html"><a href="softmax-and-multinomial-regressions.html#references-2"><i class="fa fa-check"></i><b>5.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient-descent.html"><a href="gradient-descent.html"><i class="fa fa-check"></i><b>6</b> Gradient Descent</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient-descent.html"><a href="gradient-descent.html#example-on-functions"><i class="fa fa-check"></i><b>6.1</b> Example on functions</a></li>
<li class="chapter" data-level="6.2" data-path="gradient-descent.html"><a href="gradient-descent.html#example-on-regressions"><i class="fa fa-check"></i><b>6.2</b> Example on regressions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="knnchapter.html"><a href="knnchapter.html"><i class="fa fa-check"></i><b>7</b> KNN - K Nearest Neighbour</a><ul>
<li class="chapter" data-level="7.1" data-path="knnchapter.html"><a href="knnchapter.html#example-1.-prostate-cancer-dataset"><i class="fa fa-check"></i><b>7.1</b> Example 1. Prostate Cancer dataset</a></li>
<li class="chapter" data-level="7.2" data-path="knnchapter.html"><a href="knnchapter.html#example-2.-wine-dataset"><i class="fa fa-check"></i><b>7.2</b> Example 2. Wine dataset</a><ul>
<li class="chapter" data-level="7.2.1" data-path="knnchapter.html"><a href="knnchapter.html#understand-the-data"><i class="fa fa-check"></i><b>7.2.1</b> Understand the data</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="knnchapter.html"><a href="knnchapter.html#references-3"><i class="fa fa-check"></i><b>7.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>8</b> Principal Component Analysis</a><ul>
<li class="chapter" data-level="8.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-on-an-easy-example."><i class="fa fa-check"></i><b>8.1</b> PCA on an easy example.</a></li>
<li class="chapter" data-level="8.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#references."><i class="fa fa-check"></i><b>8.2</b> References.</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="trees-random-forests-and-classification.html"><a href="trees-random-forests-and-classification.html"><i class="fa fa-check"></i><b>9</b> Trees, Random forests and Classification</a><ul>
<li class="chapter" data-level="9.1" data-path="trees-random-forests-and-classification.html"><a href="trees-random-forests-and-classification.html#introduction-1"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="trees-random-forests-and-classification.html"><a href="trees-random-forests-and-classification.html#first-example."><i class="fa fa-check"></i><b>9.2</b> First example.</a></li>
<li class="chapter" data-level="9.3" data-path="trees-random-forests-and-classification.html"><a href="trees-random-forests-and-classification.html#second-example."><i class="fa fa-check"></i><b>9.3</b> Second Example.</a></li>
<li class="chapter" data-level="9.4" data-path="trees-random-forests-and-classification.html"><a href="trees-random-forests-and-classification.html#how-does-a-tree-decide-where-to-split"><i class="fa fa-check"></i><b>9.4</b> How does a tree decide where to split?</a></li>
<li class="chapter" data-level="9.5" data-path="trees-random-forests-and-classification.html"><a href="trees-random-forests-and-classification.html#third-example."><i class="fa fa-check"></i><b>9.5</b> Third example.</a></li>
<li class="chapter" data-level="9.6" data-path="trees-random-forests-and-classification.html"><a href="trees-random-forests-and-classification.html#references-4"><i class="fa fa-check"></i><b>9.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="model-evaluation.html"><a href="model-evaluation.html"><i class="fa fa-check"></i><b>10</b> Model Evaluation</a><ul>
<li class="chapter" data-level="10.1" data-path="model-evaluation.html"><a href="model-evaluation.html#biais-variance-tradeoff"><i class="fa fa-check"></i><b>10.1</b> Biais variance tradeoff</a></li>
<li class="chapter" data-level="10.2" data-path="model-evaluation.html"><a href="model-evaluation.html#bagging"><i class="fa fa-check"></i><b>10.2</b> Bagging</a></li>
<li class="chapter" data-level="10.3" data-path="model-evaluation.html"><a href="model-evaluation.html#crossvalidation"><i class="fa fa-check"></i><b>10.3</b> Cross Validation</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="titanic.html"><a href="titanic.html"><i class="fa fa-check"></i><b>11</b> Case Study - Predicting Survivalship on the Titanic</a><ul>
<li class="chapter" data-level="11.1" data-path="titanic.html"><a href="titanic.html#import-the-data."><i class="fa fa-check"></i><b>11.1</b> Import the data.</a></li>
<li class="chapter" data-level="11.2" data-path="titanic.html"><a href="titanic.html#tidy-the-data"><i class="fa fa-check"></i><b>11.2</b> Tidy the data</a></li>
<li class="chapter" data-level="11.3" data-path="titanic.html"><a href="titanic.html#understand-the-data-1"><i class="fa fa-check"></i><b>11.3</b> Understand the data</a><ul>
<li class="chapter" data-level="11.3.1" data-path="titanic.html"><a href="titanic.html#a.-transform-the-data"><i class="fa fa-check"></i><b>11.3.1</b> A. Transform the data</a></li>
<li class="chapter" data-level="11.3.2" data-path="titanic.html"><a href="titanic.html#a.-vizualize-with-families."><i class="fa fa-check"></i><b>11.3.2</b> A. Vizualize with families.</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="titanic.html"><a href="titanic.html#a.-visualize-with-cabins."><i class="fa fa-check"></i><b>11.4</b> A. Visualize with cabins.</a></li>
<li class="chapter" data-level="11.5" data-path="titanic.html"><a href="titanic.html#b.-transform-dealing-with-missing-data."><i class="fa fa-check"></i><b>11.5</b> B. Transform Dealing with missing data.</a><ul>
<li class="chapter" data-level="11.5.1" data-path="titanic.html"><a href="titanic.html#overview."><i class="fa fa-check"></i><b>11.5.1</b> Overview.</a></li>
<li class="chapter" data-level="11.5.2" data-path="titanic.html"><a href="titanic.html#c.-transform-more-feature-engineering-with-the-ages-and-others."><i class="fa fa-check"></i><b>11.5.2</b> C. Transform More feature engineering with the ages and others.</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="titanic.html"><a href="titanic.html#references.-1"><i class="fa fa-check"></i><b>11.6</b> References.</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="mushroom.html"><a href="mushroom.html"><i class="fa fa-check"></i><b>12</b> Case Study - Mushrooms Classification</a><ul>
<li class="chapter" data-level="12.1" data-path="mushroom.html"><a href="mushroom.html#import-the-data"><i class="fa fa-check"></i><b>12.1</b> Import the data</a></li>
<li class="chapter" data-level="12.2" data-path="mushroom.html"><a href="mushroom.html#tidy-the-data-1"><i class="fa fa-check"></i><b>12.2</b> Tidy the data</a></li>
<li class="chapter" data-level="12.3" data-path="mushroom.html"><a href="mushroom.html#understand-the-data-2"><i class="fa fa-check"></i><b>12.3</b> Understand the data</a><ul>
<li class="chapter" data-level="12.3.1" data-path="mushroom.html"><a href="mushroom.html#transform-the-data"><i class="fa fa-check"></i><b>12.3.1</b> Transform the data</a></li>
<li class="chapter" data-level="12.3.2" data-path="mushroom.html"><a href="mushroom.html#visualize-the-data"><i class="fa fa-check"></i><b>12.3.2</b> Visualize the data</a></li>
<li class="chapter" data-level="12.3.3" data-path="mushroom.html"><a href="mushroom.html#modeling"><i class="fa fa-check"></i><b>12.3.3</b> Modeling</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="mushroom.html"><a href="mushroom.html#communication"><i class="fa fa-check"></i><b>12.4</b> Communication</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="breastcancer.html"><a href="breastcancer.html"><i class="fa fa-check"></i><b>13</b> Case Study - Wisconsin Breast Cancer</a><ul>
<li class="chapter" data-level="13.1" data-path="breastcancer.html"><a href="breastcancer.html#import-the-data-1"><i class="fa fa-check"></i><b>13.1</b> Import the data</a></li>
<li class="chapter" data-level="13.2" data-path="breastcancer.html"><a href="breastcancer.html#tidy-the-data-2"><i class="fa fa-check"></i><b>13.2</b> Tidy the data</a></li>
<li class="chapter" data-level="13.3" data-path="breastcancer.html"><a href="breastcancer.html#understand-the-data-3"><i class="fa fa-check"></i><b>13.3</b> Understand the data</a><ul>
<li class="chapter" data-level="13.3.1" data-path="breastcancer.html"><a href="breastcancer.html#transform-the-data-1"><i class="fa fa-check"></i><b>13.3.1</b> Transform the data</a></li>
<li class="chapter" data-level="13.3.2" data-path="breastcancer.html"><a href="breastcancer.html#pre-process-the-data"><i class="fa fa-check"></i><b>13.3.2</b> Pre-process the data</a></li>
<li class="chapter" data-level="13.3.3" data-path="breastcancer.html"><a href="breastcancer.html#model-the-data-1"><i class="fa fa-check"></i><b>13.3.3</b> Model the data</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="breastcancer.html"><a href="breastcancer.html#references-5"><i class="fa fa-check"></i><b>13.4</b> References</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mlr" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Single &amp; Multiple Linear Regression</h1>
<div id="single-variable-regression" class="section level2">
<h2><span class="header-section-number">3.1</span> Single variable regression</h2>
<p>The general equation for a linear regression model</p>
<blockquote>
<p><span class="math inline">\(y^i = \beta_{0} + \beta_{1} x^i + \epsilon^i\)</span></p>
</blockquote>
<p>where:</p>
<ul>
<li><span class="math inline">\(y^i\)</span> is the <span class="math inline">\(i^{th}\)</span> observation of the dependent variable</li>
<li><span class="math inline">\(\beta_{0}\)</span> is the intercept coefficient</li>
<li><span class="math inline">\(\beta_{1}\)</span> is the regression coefficient for the dependent variable</li>
<li><span class="math inline">\(x^i\)</span> is the <span class="math inline">\(i^{th}\)</span> observation of the independent variable</li>
<li><span class="math inline">\(\epsilon^i\)</span> is the error term for the <span class="math inline">\(i^{th}\)</span> observation. It basically is the difference in therm of y between the observed value and the estimated value. It is also called the residuals. A good model minimize these errors.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
</ul>
<p>Some ways to assess how good our model is to:</p>
<ol style="list-style-type: decimal">
<li>compute the SSE (the sum of squared error)
<ul>
<li>SSE = <span class="math inline">\((\epsilon^1)^2 + (\epsilon^2)^2 + \ldots + (\epsilon^n)^2\)</span> = <span class="math inline">\(\sum_{i=1}^N \epsilon^i\)</span></li>
<li>A good model will minimize SSE</li>
<li>problem: SSE is dependent of N. SSE will naturally increase as N increase</li>
</ul></li>
<li>compute the RMSE (the root mean squared error)
<ul>
<li>RMSE = <span class="math inline">\(\sqrt {\frac {SSE} {N}}\)</span></li>
<li>Also a good model will minimize SSE</li>
<li>It depends of the unit of the dependent variable. It is like the average error the model is making (in term of the unit of the dependent variable)</li>
</ul></li>
<li>compute <span class="math inline">\(R^2\)</span>
<ul>
<li>It compare the models to a baseline model</li>
<li><span class="math inline">\(R^2\)</span> is <strong>unitless</strong> and <strong>universaly</strong> interpretable</li>
<li>SST is the sum of the squared of the difference between the observed value and the mean of all the observed value</li>
<li><span class="math inline">\(R^2 = 1 - \frac {SSE} {SST}\)</span></li>
</ul></li>
</ol>
<p>We usually use r-squared to check the performance of a regression.</p>
<p>The conditions and assumptions to have a valid linear model are the same as for the t-test.</p>
<ul>
<li>linear relationship between dependent and independent variables. (scatterplot of dependent vs independent variables + scatterplot of residuals vs fitted). Also check here for outliers. Regression line and coefficient of regression are affected by outliers. Check it would make sense to remove them.<br />
</li>
<li>Multivariate normality. Multiple regression assumes that the residuals are normally distributed. Visual check on the Q-Q plot.<br />
</li>
<li>No Multicollinearity. Multiple regression assumes that the independent variables are not highly correlated with each other. Check correlation matrix and correlation plot. This assumption can also be tested using Variance Inflation Factor (VIF) values.</li>
<li>Homoscedasticity. This assumption states that the variance of error terms are similar across the values of the independent variables. A plot of standardized residuals versus predicted values can show whether points are equally distributed across all values of the independent variables.</li>
</ul>
<p>In our first linear regression, weâll use the <strong>Wine</strong> dataset. Letâs load it and then have a quick look at its structure. </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df =<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;dataset/Wine.csv&quot;</span>)
<span class="kw">skim</span>(df)</code></pre></div>
<pre><code>## Skim summary statistics
##  n obs: 25 
##  n variables: 7 
## 
## Variable type: integer 
##           var missing complete  n    mean     sd  min  p25 median  p75
## 1         Age       0       25 25   17.2    7.69    5   11     17   23
## 2 HarvestRain       0       25 25  148.56  74.42   38   89    130  187
## 3  WinterRain       0       25 25  605.28 132.28  376  536    600  697
## 4        Year       0       25 25 1965.8    7.69 1952 1960   1966 1972
##    max     hist
## 1   31 ââââââââ
## 2  292 ââââââââ
## 3  830 ââââââââ
## 4 1978 ââââââââ
## 
## Variable type: numeric 
##         var missing complete  n     mean      sd      min      p25
## 1      AGST       0       25 25    16.51    0.68    14.98    16.2 
## 2 FrancePop       0       25 25 49694.44 3665.27 43183.57 46584   
## 3     Price       0       25 25     7.07    0.65     6.2      6.52
##     median      p75      max     hist
## 1    16.53    17.07    17.65 ââââââââ
## 2 50254.97 52894.18 54602.19 ââââââââ
## 3     7.12     7.5      8.49 ââââââââ</code></pre>
<p>We use the <code>lm</code> function to create our linear regression model. We use <em>AGST</em> as the independent variable while the <em>price</em> is the dependent variable. <img src="machinelearningwithR_files/figure-html/linreg02-plot-1.png" width="672" /></p>
<p>We can see a weak positive correlation between <code>AGST</code> and <code>Price</code>. The model would confirm that.<br />
</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_lm_df =<span class="st"> </span><span class="kw">lm</span>(Price <span class="op">~</span><span class="st"> </span>AGST, <span class="dt">data =</span> df)
<span class="kw">summary</span>(model_lm_df)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Price ~ AGST, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.78450 -0.23882 -0.03727  0.38992  0.90318 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -3.4178     2.4935  -1.371 0.183710    
## AGST          0.6351     0.1509   4.208 0.000335 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4993 on 23 degrees of freedom
## Multiple R-squared:  0.435,  Adjusted R-squared:  0.4105 
## F-statistic: 17.71 on 1 and 23 DF,  p-value: 0.000335</code></pre>
<p>The <code>summary</code> function applied on the model is giving us important information. See below for a detailed explanation of it.</p>
<ul>
<li>the stars next to the predictor variable indicated how significant the variable is for our regression model</li>
<li>it also gives us the value of the R^2 coefficient</li>
</ul>
<p>We could have calculated the R^2 value in this way: </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">SSE =<span class="st"> </span><span class="kw">sum</span>(model_lm_df<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)
SST =<span class="st"> </span><span class="kw">sum</span>((df<span class="op">$</span>Price <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(df<span class="op">$</span>Price))<span class="op">^</span><span class="dv">2</span>)
r_squared =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>SSE<span class="op">/</span>SST
r_squared</code></pre></div>
<pre><code>## [1] 0.4350232</code></pre>
<p>The low R^2 indicate our model does not explain much of the variance of the data.</p>
<p>We can now plot the observations and the line of regression; and see how the linear model fits the data. </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(df, <span class="kw">aes</span>(AGST, Price)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="machinelearningwithR_files/figure-html/linreg04-graph-1.png" width="672" /> By default, the <code>geom_smooth()</code> will use a 95% confidence interval (which is the grey-er area on the graph). There are 95% chance the line of regression will be within that zone for the whole population.</p>
<p>It is always nice to see how our residuals are distributed.<br />
We use the <code>ggplot2</code> library and the <code>fortify</code> function which transform the <code>summary(model1)</code> into a data frame usable for plotting.   </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model1 &lt;-<span class="st"> </span><span class="kw">fortify</span>(model_lm_df)
p &lt;-<span class="st"> </span><span class="kw">ggplot</span>(model1, <span class="kw">aes</span>(.fitted, .resid)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() 
p &lt;-<span class="st"> </span>p <span class="op">+</span><span class="st"> </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) 
p &lt;-<span class="st"> </span>p <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted values&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Residuals&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Plot of the residuals in function of the fitted values&quot;</span>)
p</code></pre></div>
<p><img src="machinelearningwithR_files/figure-html/linreg05_residuals-1.png" width="672" /></p>
<p>Residuals look normal: randonly scattered around the zero line.</p>
</div>
<div id="multi-variables-regression" class="section level2">
<h2><span class="header-section-number">3.2</span> Multi-variables regression</h2>
<p>Instead of just considering one variable as predictor, weâll add a few more variables to our model with the idea to increase its predictive ability. In our case, we are expecting an increased r-squared value.</p>
<p>We have to be cautious in adding more variables. Too many variable might give a high <span class="math inline">\(R^2\)</span> on our training data, but this not be the case as we switch to our testing data. This is because of over-fitting and we will need to avoid this at all cost. Weâll check several ways we can use against overfitting.</p>
<p>The general equations can be expressed as</p>
<blockquote>
<p><span class="math inline">\(y^i = \beta_{0} + \beta_{1} x_{1}^i + \beta_{2} x_{2}^i + \ldots + \beta_{k} x_{k}^i + \epsilon^i\)</span></p>
</blockquote>
<p>when there are k predictors variables.</p>
<p>There are a bit of trials and errors to make while trying to fit multiple variables into a model, but a rule of thumb would be to include most of the variable (all these that would make sense) and then take out the ones that are not very significant using the <code>summary(modelx)</code></p>
<p>We are introducing 3 news libraries here besides the usual tidyverse.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(corrr)
<span class="kw">library</span>(corrplot)
<span class="kw">library</span>(leaps)</code></pre></div>
<div id="predicting-wine-price-again" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Predicting wine price (again!)</h3>
<p>We continue here with the same dataset, <em>wine.csv</em>.<br />
First, we can see how each variable is correlated with each other ones.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(corrr)
d &lt;-<span class="st"> </span><span class="kw">correlate</span>(df)
d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">shave</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fashion</span>()</code></pre></div>
<pre><code>##       rowname  Year Price WinterRain AGST HarvestRain  Age FrancePop
## 1        Year                                                       
## 2       Price  -.45                                                 
## 3  WinterRain   .02   .14                                           
## 4        AGST  -.25   .66       -.32                                
## 5 HarvestRain   .03  -.56       -.28 -.06                           
## 6         Age -1.00   .45       -.02  .25        -.03               
## 7   FrancePop   .99  -.47       -.00 -.26         .04 -.99</code></pre>
<p>By default, R uses the Pearson coefficient of correlation.</p>
<p>Multiple linear regression doesnât handle well multicollinearity. In this case, we should remove variables that are too highly correlated. <em>Age</em> and <em>Year</em> are too highly correlated and it should be removed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">corrplot<span class="op">::</span><span class="kw">corrplot</span>(<span class="kw">cor</span>(df), <span class="dt">type =</span> <span class="st">&quot;lower&quot;</span>, <span class="dt">order =</span> <span class="st">&quot;hclust&quot;</span>, <span class="dt">tl.col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">sig.level =</span> <span class="fl">0.01</span>)</code></pre></div>
<p><img src="machinelearningwithR_files/figure-html/mlr-corrplot01-1.png" width="672" /></p>
<p>So letâs start by using all variables.<br />
</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model2_lm_df &lt;-<span class="st"> </span><span class="kw">lm</span>(Price <span class="op">~</span><span class="st"> </span>Year <span class="op">+</span><span class="st"> </span>WinterRain <span class="op">+</span><span class="st"> </span>AGST <span class="op">+</span><span class="st"> </span>HarvestRain <span class="op">+</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>FrancePop, <span class="dt">data =</span> df)
<span class="kw">summary</span>(model2_lm_df)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Price ~ Year + WinterRain + AGST + HarvestRain + 
##     Age + FrancePop, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.48179 -0.24662 -0.00726  0.22012  0.51987 
## 
## Coefficients: (1 not defined because of singularities)
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  7.092e-01  1.467e+02   0.005 0.996194    
## Year        -5.847e-04  7.900e-02  -0.007 0.994172    
## WinterRain   1.043e-03  5.310e-04   1.963 0.064416 .  
## AGST         6.012e-01  1.030e-01   5.836 1.27e-05 ***
## HarvestRain -3.958e-03  8.751e-04  -4.523 0.000233 ***
## Age                 NA         NA      NA       NA    
## FrancePop   -4.953e-05  1.667e-04  -0.297 0.769578    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3019 on 19 degrees of freedom
## Multiple R-squared:  0.8294, Adjusted R-squared:  0.7845 
## F-statistic: 18.47 on 5 and 19 DF,  p-value: 1.044e-06</code></pre>
<p>While doing so, we notice that the variable <em>Age</em> has NA. This is because it is so highly correlated with the variable <em>year</em> and <em>FrancePop</em>. This came in from our correlation plot. Also the variable <em>FrancePop</em> isnât very predictive of the price of wine. So we can refine our models, by taking out these 2 variables, and as weâll see, it wonât affect much our <span class="math inline">\(R^2\)</span> value. Note that with multiple variables regression, it is important to look at the <strong>Adjusted R-squared</strong> as it take into consideration the amount of variables in the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model3_lm_df &lt;-<span class="st"> </span><span class="kw">lm</span>(Price <span class="op">~</span><span class="st"> </span>Year <span class="op">+</span><span class="st"> </span>WinterRain <span class="op">+</span><span class="st"> </span>AGST <span class="op">+</span><span class="st"> </span>HarvestRain, <span class="dt">data =</span> df)
<span class="kw">summary</span>(model3_lm_df)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Price ~ Year + WinterRain + AGST + HarvestRain, 
##     data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.45470 -0.24273  0.00752  0.19773  0.53637 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 44.0248601 16.4434570   2.677 0.014477 *  
## Year        -0.0239308  0.0080969  -2.956 0.007819 ** 
## WinterRain   0.0010755  0.0005073   2.120 0.046694 *  
## AGST         0.6072093  0.0987022   6.152  5.2e-06 ***
## HarvestRain -0.0039715  0.0008538  -4.652 0.000154 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.295 on 20 degrees of freedom
## Multiple R-squared:  0.8286, Adjusted R-squared:  0.7943 
## F-statistic: 24.17 on 4 and 20 DF,  p-value: 2.036e-07</code></pre>
<p>We managed now to have a better r-squared than using only one predictive variable. Also by choosing better predictive variables we managed to increase our <em>adjusted r-squared</em>.</p>
<p>Although it isnât now feasible to graph in 2D the <em>Price</em> in function of the other variables, we can still graph our residuals in 2D. </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model3 &lt;-<span class="st"> </span><span class="kw">fortify</span>(model3_lm_df)
p &lt;-<span class="st"> </span><span class="kw">ggplot</span>(model3, <span class="kw">aes</span>(.fitted, .resid)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted values&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Residuals&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Plot of the residuals in function of the fitted values (multiple variables)&quot;</span>)
p</code></pre></div>
<p><img src="machinelearningwithR_files/figure-html/linreg07-1.png" width="672" /></p>
<p>The plot of residuals look pretty normal with points randomly scattered around the 0 line.</p>
</div>
</div>
<div id="model-diagnostic-and-evaluation" class="section level2">
<h2><span class="header-section-number">3.3</span> Model diagnostic and evaluation</h2>
<p>Letâs first on onto the explanations of the summary function on the regression model.</p>
<p><strong>Call:</strong> The formula we have used for our model.</p>
<p><strong>Coefficient â Estimate</strong> The coefficient Estimate is the value of the coefficient that is to be used in the equation. The coefficients for each of the independent variable has a meaning, for example, 0.0010755 for âWinterRainâ means that for every 1 unit change in âWinterRainâ, the value of âPriceâ increases by 0.0010755.</p>
<p><strong>Coefficient â Standard Error</strong> The coefficient Standard Error measures the average amount that the coefficient estimates vary from the actual average value of our response variable. We need this to be minimal for the variable to be able to predict accurately.</p>
<p><strong>Coefficient â t value:</strong> The coefficient t-value measures how many standard deviations our coefficient estimate can be far away from 0. We want this value to be high so that we can reject the null hypothesis (H0) which is âthere is no relationship between dependent and independent variablesâ.</p>
<p><strong>Coefficient â Pr(&gt;t):</strong> The Pr(&gt;t) is computed from the t values. This is used for rejecting the Null Hypothesis (H00) stated above. Normally, the value for this less than 0.05 or 5% is considered to be the cut-off point for rejecting H0.</p>
<p><strong>Residuals:</strong> Residuals are the next component in the model summary. Residuals are the difference between the predicted values by the model and the actual values in the dataset. For the model to be good, the residuals should be normally distributed.</p>
<p><strong>Adjusted R-squared:</strong><br />
Adjusted R-squared is considered for evaluating model accuracy when the number of independent variables is greater than 1. Adjusted R-squared adjusts the number of variables considered in the model and is the preferred measure for evaluating the model goodness.</p>
<p><strong>F-Statistic:</strong> F-statistic is used for finding out if there exists any relationship between our independent (predictor) and the dependent (response) variables. Normally, the value of F-statistic greater than one can be used for rejecting the null hypothesis (H0: There is no relationship between Employed and other independent variables). For our model, the value of F-statistic, 330.6 is very high because of the limited data points. The p-value in the output for F-statistic is evaluated the same way we use the Pr(&gt;t) value in the coefficients output. For the p-value, we can reject the null hypothesis (H0) as p-value &lt; 0.05.</p>
<p><strong>There is no established relationship between the two.</strong> R-squared tells how much variation in the response variable is explained by the predictor variables while p-value tells if the predictors used in the model are able to explain the response variable or not. If p-value &lt; 0.05 (for 95% confidence), then the model is considered to be good.</p>
<ol style="list-style-type: decimal">
<li><strong>low R-square</strong> and <strong>low p-value</strong> (p-value &lt;= 0.05): This means that the model doesnât explain much of the variation in the response variable, but still this is considered better than having no model to explain the response variable as it is significant as per the p-value.</li>
<li><strong>low R-square</strong> and <strong>high p-value</strong> (p-value &gt; 0.05): This means that model doesnât explain much variation in the data and is not significant. We should discard such model as this is the worst scenario.</li>
<li><strong>high R-square</strong> and <strong>low p-value</strong>: This means that model explains a lot of variation in the data and is also significant. This scenario is best of the four and the model is considered to be good in this case.</li>
<li><strong>high R-square</strong> and <strong>high p-value</strong>: This means that variance in the data is explained by the model but it is not significant. We should not use such model for predictions.</li>
</ol>
<p>Here are the nessary conditions for a linear regression model to be valid. Hence, these are the assumptions made when doing a linear regression.</p>
<ul>
<li><strong>Linear Relationship</strong>.<br />
The plot of the residuals should show the data points randomly scattered around the 0 line.<br />
This plot shows if residuals have non-linear patterns. There could be a non-linear relationship between predictor variables and an outcome variable and the pattern could show up in this plot if the model doesnât capture the non-linear relationship. If you find equally spread residuals around a horizontal line without distinct patterns, that is a good indication you donât have non-linear relationships.</li>
</ul>
<div class="figure">
<img src="otherpics/GoodVsBadResidualsPlot.png" alt="Good Vs Bad residuals plot" />
<p class="caption">Good Vs Bad residuals plot</p>
</div>
<p>There isnât any distinctive pattern in Case 1, but there is a parabola in Case 2, where the non-linear relationship was not explained by the model and was left out in the residuals.</p>
<ul>
<li><p><strong>Multivariate normality</strong>. The multiple linear regression analysis requires that the errors between observed and predicted values (i.e., the residuals of the regression) should be normally distributed. This assumption may be checked by looking at a histogram or a Q-Q-Plot. Normality can also be checked with a goodness of fit test (e.g., the Kolmogorov-Smirnov test), though this test must be conducted on the residuals themselves.<br />
<img src="otherpics/GoodVsBadQQPlot.png" alt="Good Vs Bad residuals Q-Q plot" /></p></li>
<li><strong>No Multicollinearity</strong>. Multicollinearity may be tested with these central criteria:</li>
</ul>
<ol style="list-style-type: decimal">
<li>Correlation matrix. When computing the matrix of Pearsonâs Bivariate Correlation among all independent variables the correlation coefficients need to be smaller than 1.</li>
<li>Variance Inflation Factor (VIF) â the variance inflation factor of the linear regression is defined as VIF = 1/T. Tolerance (T) is defined as T = 1 â RÂ². With VIF &gt; 10 there is an indication that multicollinearity may be present; with VIF &gt; 100 there is certainly multicollinearity among the variables. If multicollinearity is found in the data, centering the data (that is deducting the mean of the variable from each score) might help to solve the problem. However, the simplest way to address the problem is to remove independent variables with high VIF values.</li>
</ol>
<ul>
<li><strong>Homoscedasticity</strong>. A scatterplot of residuals versus predicted values is good way to check for homoscedasticity. There should be no clear pattern in the distribution; if there is a cone-shaped pattern (as shown below), the data is heteroscedastic. If the data are heteroscedastic, a non-linear data transformation or addition of a quadratic term might fix the problem.</li>
</ul>
<p>This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). Itâs good if you see a horizontal line with equally (randomly) spread points.<br />
<img src="otherpics/GoodVsBadScalePlot.png" alt="Good Vs Bad residuals Q-Q plot" /></p>
<p>In Case 2, the residuals begin to spread wider along the x-axis. Because the residuals spread wider and wider, the red smooth line is not horizontal and shows a steep angle in Case 2.</p>
</div>
<div id="final-example---boston-dataset---with-backward-elimination" class="section level2">
<h2><span class="header-section-number">3.4</span> Final example - Boston dataset - with backward elimination</h2>
<p>On this last example weâll use a more systemic way to find out which variables should be chosen into our models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;dataset/Boston.csv&quot;</span>)
skimr<span class="op">::</span><span class="kw">skim</span>(df)</code></pre></div>
<pre><code>## Skim summary statistics
##  n obs: 506 
##  n variables: 14 
## 
## Variable type: integer 
##    var missing complete   n    mean     sd min p25 median p75 max     hist
## 1 CHAS       0      506 506   0.069   0.25   0   0      0   0   1 ââââââââ
## 2  RAD       0      506 506   9.55    8.71   1   4      5  24  24 ââââââââ
## 3  TAX       0      506 506 408.24  168.54 187 279    330 666 711 ââââââââ
## 
## Variable type: numeric 
##        var missing complete   n   mean    sd     min     p25 median    p75
## 1      AGE       0      506 506  68.57 28.15  2.9     45.02   77.5   94.07
## 2        B       0      506 506 356.67 91.29  0.32   375.38  391.44 396.23
## 3    Crime       0      506 506   3.61  8.6   0.0063   0.082   0.26   3.68
## 4      DIS       0      506 506   3.8   2.11  1.13     2.1     3.21   5.19
## 5    INDUS       0      506 506  11.14  6.86  0.46     5.19    9.69  18.1 
## 6    LSTAT       0      506 506  12.65  7.14  1.73     6.95   11.36  16.96
## 7     MEDV       0      506 506  22.53  9.2   5       17.02   21.2   25   
## 8      NOX       0      506 506   0.55  0.12  0.39     0.45    0.54   0.62
## 9  PTRATIO       0      506 506  18.46  2.16 12.6     17.4    19.05  20.2 
## 10      RM       0      506 506   6.28  0.7   3.56     5.89    6.21   6.62
## 11      ZN       0      506 506  11.36 23.32  0        0       0     12.5 
##       max     hist
## 1  100    ââââââââ
## 2  396.9  ââââââââ
## 3   88.98 ââââââââ
## 4   12.13 ââââââââ
## 5   27.74 ââââââââ
## 6   37.97 ââââââââ
## 7   50    ââââââââ
## 8    0.87 ââââââââ
## 9   22    ââââââââ
## 10   8.78 ââââââââ
## 11 100    ââââââââ</code></pre>
<p>Here is the list of variables with their meaning.</p>
<ul>
<li>CRIM per capita crime rate by town</li>
<li>ZN proportion of residential land zoned for lots over 25,000 sq.ft.</li>
<li>INDUS proportion of non-retail business acres per town</li>
<li>CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</li>
<li>NOX nitric oxides concentration (parts per 10 million)</li>
<li>RM average number of rooms per dwelling</li>
<li>AGE proportion of owner-occupied units built prior to 1940</li>
<li>DIS weighted distances to five Boston employment centres</li>
<li>RAD index of accessibility to radial highways</li>
<li>TAX full-value property-tax rate per $10,000</li>
<li>PTRATIO pupil-teacher ratio by town</li>
<li>B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town</li>
<li>LSTAT % lower status of the population</li>
<li>MEDV Median value of owner-occupied homes in $1000âs</li>
</ul>
<p>Letâs make the necessary adjustment in variable types</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df<span class="op">$</span>CHAS &lt;-<span class="st"> </span><span class="kw">factor</span>(df<span class="op">$</span>CHAS)</code></pre></div>
<p>A quick check on how correlated are our variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">corrplot</span>(<span class="kw">cor</span>(df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>CHAS)), <span class="dt">type =</span> <span class="st">&quot;lower&quot;</span>, <span class="dt">order =</span> <span class="st">&quot;hclust&quot;</span>, <span class="dt">tl.col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">sig.level =</span> <span class="fl">0.01</span>)</code></pre></div>
<p><img src="machinelearningwithR_files/figure-html/linreg09-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">correlate</span>(df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>CHAS)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">shave</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fashion</span>()</code></pre></div>
<pre><code>##    rowname Crime   ZN INDUS  NOX   RM  AGE  DIS  RAD  TAX PTRATIO    B
## 1    Crime                                                            
## 2       ZN  -.20                                                      
## 3    INDUS   .41 -.53                                                 
## 4      NOX   .42 -.52   .76                                           
## 5       RM  -.22  .31  -.39 -.30                                      
## 6      AGE   .35 -.57   .64  .73 -.24                                 
## 7      DIS  -.38  .66  -.71 -.77  .21 -.75                            
## 8      RAD   .63 -.31   .60  .61 -.21  .46 -.49                       
## 9      TAX   .58 -.31   .72  .67 -.29  .51 -.53  .91                  
## 10 PTRATIO   .29 -.39   .38  .19 -.36  .26 -.23  .46  .46             
## 11       B  -.39  .18  -.36 -.38  .13 -.27  .29 -.44 -.44    -.18     
## 12   LSTAT   .46 -.41   .60  .59 -.61  .60 -.50  .49  .54     .37 -.37
## 13    MEDV  -.39  .36  -.48 -.43  .70 -.38  .25 -.38 -.47    -.51  .33
##    LSTAT MEDV
## 1            
## 2            
## 3            
## 4            
## 5            
## 6            
## 7            
## 8            
## 9            
## 10           
## 11           
## 12           
## 13  -.74</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">yo &lt;-<span class="st"> </span><span class="kw">correlate</span>(df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>CHAS)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">shave</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fashion</span>()
<span class="kw">kable</span>(yo, <span class="dt">format =</span> <span class="st">&quot;html&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable_styling</span>()</code></pre></div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
rowname
</th>
<th style="text-align:left;">
Crime
</th>
<th style="text-align:left;">
ZN
</th>
<th style="text-align:left;">
INDUS
</th>
<th style="text-align:left;">
NOX
</th>
<th style="text-align:left;">
RM
</th>
<th style="text-align:left;">
AGE
</th>
<th style="text-align:left;">
DIS
</th>
<th style="text-align:left;">
RAD
</th>
<th style="text-align:left;">
TAX
</th>
<th style="text-align:left;">
PTRATIO
</th>
<th style="text-align:left;">
B
</th>
<th style="text-align:left;">
LSTAT
</th>
<th style="text-align:left;">
MEDV
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Crime
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
ZN
</td>
<td style="text-align:left;">
-.20
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
INDUS
</td>
<td style="text-align:left;">
.41
</td>
<td style="text-align:left;">
-.53
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
NOX
</td>
<td style="text-align:left;">
.42
</td>
<td style="text-align:left;">
-.52
</td>
<td style="text-align:left;">
.76
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
RM
</td>
<td style="text-align:left;">
-.22
</td>
<td style="text-align:left;">
.31
</td>
<td style="text-align:left;">
-.39
</td>
<td style="text-align:left;">
-.30
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
AGE
</td>
<td style="text-align:left;">
.35
</td>
<td style="text-align:left;">
-.57
</td>
<td style="text-align:left;">
.64
</td>
<td style="text-align:left;">
.73
</td>
<td style="text-align:left;">
-.24
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
DIS
</td>
<td style="text-align:left;">
-.38
</td>
<td style="text-align:left;">
.66
</td>
<td style="text-align:left;">
-.71
</td>
<td style="text-align:left;">
-.77
</td>
<td style="text-align:left;">
.21
</td>
<td style="text-align:left;">
-.75
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
RAD
</td>
<td style="text-align:left;">
.63
</td>
<td style="text-align:left;">
-.31
</td>
<td style="text-align:left;">
.60
</td>
<td style="text-align:left;">
.61
</td>
<td style="text-align:left;">
-.21
</td>
<td style="text-align:left;">
.46
</td>
<td style="text-align:left;">
-.49
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
TAX
</td>
<td style="text-align:left;">
.58
</td>
<td style="text-align:left;">
-.31
</td>
<td style="text-align:left;">
.72
</td>
<td style="text-align:left;">
.67
</td>
<td style="text-align:left;">
-.29
</td>
<td style="text-align:left;">
.51
</td>
<td style="text-align:left;">
-.53
</td>
<td style="text-align:left;">
.91
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
PTRATIO
</td>
<td style="text-align:left;">
.29
</td>
<td style="text-align:left;">
-.39
</td>
<td style="text-align:left;">
.38
</td>
<td style="text-align:left;">
.19
</td>
<td style="text-align:left;">
-.36
</td>
<td style="text-align:left;">
.26
</td>
<td style="text-align:left;">
-.23
</td>
<td style="text-align:left;">
.46
</td>
<td style="text-align:left;">
.46
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
B
</td>
<td style="text-align:left;">
-.39
</td>
<td style="text-align:left;">
.18
</td>
<td style="text-align:left;">
-.36
</td>
<td style="text-align:left;">
-.38
</td>
<td style="text-align:left;">
.13
</td>
<td style="text-align:left;">
-.27
</td>
<td style="text-align:left;">
.29
</td>
<td style="text-align:left;">
-.44
</td>
<td style="text-align:left;">
-.44
</td>
<td style="text-align:left;">
-.18
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
LSTAT
</td>
<td style="text-align:left;">
.46
</td>
<td style="text-align:left;">
-.41
</td>
<td style="text-align:left;">
.60
</td>
<td style="text-align:left;">
.59
</td>
<td style="text-align:left;">
-.61
</td>
<td style="text-align:left;">
.60
</td>
<td style="text-align:left;">
-.50
</td>
<td style="text-align:left;">
.49
</td>
<td style="text-align:left;">
.54
</td>
<td style="text-align:left;">
.37
</td>
<td style="text-align:left;">
-.37
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
MEDV
</td>
<td style="text-align:left;">
-.39
</td>
<td style="text-align:left;">
.36
</td>
<td style="text-align:left;">
-.48
</td>
<td style="text-align:left;">
-.43
</td>
<td style="text-align:left;">
.70
</td>
<td style="text-align:left;">
-.38
</td>
<td style="text-align:left;">
.25
</td>
<td style="text-align:left;">
-.38
</td>
<td style="text-align:left;">
-.47
</td>
<td style="text-align:left;">
-.51
</td>
<td style="text-align:left;">
.33
</td>
<td style="text-align:left;">
-.74
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_mlr_df &lt;-<span class="st"> </span><span class="kw">lm</span>(MEDV <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> df)
model_bwe_df &lt;-<span class="st"> </span><span class="kw">regsubsets</span>(<span class="kw">formula</span>(model_mlr_df), <span class="dt">data =</span> df, <span class="dt">method =</span> <span class="st">&quot;backward&quot;</span>)
<span class="kw">summary</span>(model_mlr_df)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MEDV ~ ., data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.595  -2.730  -0.518   1.777  26.199 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***
## Crime       -1.080e-01  3.286e-02  -3.287 0.001087 ** 
## ZN           4.642e-02  1.373e-02   3.382 0.000778 ***
## INDUS        2.056e-02  6.150e-02   0.334 0.738288    
## CHAS1        2.687e+00  8.616e-01   3.118 0.001925 ** 
## NOX         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***
## RM           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***
## AGE          6.922e-04  1.321e-02   0.052 0.958229    
## DIS         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***
## RAD          3.060e-01  6.635e-02   4.613 5.07e-06 ***
## TAX         -1.233e-02  3.760e-03  -3.280 0.001112 ** 
## PTRATIO     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***
## B            9.312e-03  2.686e-03   3.467 0.000573 ***
## LSTAT       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.745 on 492 degrees of freedom
## Multiple R-squared:  0.7406, Adjusted R-squared:  0.7338 
## F-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(model_bwe_df, <span class="dt">scale =</span> <span class="st">&quot;adjr2&quot;</span>)</code></pre></div>
<p><img src="machinelearningwithR_files/figure-html/linreg11-1.png" width="672" /></p>
<p>Ideally, the model should consider the following variables</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model2_mlr_df &lt;-<span class="st"> </span><span class="kw">lm</span>(MEDV <span class="op">~</span><span class="st"> </span>Crime <span class="op">+</span><span class="st"> </span>NOX <span class="op">+</span><span class="st"> </span>RM <span class="op">+</span><span class="st"> </span>DIS <span class="op">+</span><span class="st"> </span>RAD <span class="op">+</span><span class="st"> </span>PTRATIO <span class="op">+</span><span class="st"> </span>B <span class="op">+</span><span class="st"> </span>LSTAT, <span class="dt">data =</span> df)
<span class="kw">summary</span>(model2_mlr_df)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MEDV ~ Crime + NOX + RM + DIS + RAD + PTRATIO + 
##     B + LSTAT, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.0482  -3.0662  -0.5636   1.8869  26.7061 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  35.691726   5.179826   6.891 1.69e-11 ***
## Crime        -0.102920   0.033516  -3.071 0.002252 ** 
## NOX         -20.141922   3.517282  -5.727 1.78e-08 ***
## RM            4.174645   0.411001  10.157  &lt; 2e-16 ***
## DIS          -1.214760   0.165053  -7.360 7.68e-13 ***
## RAD           0.145792   0.041132   3.544 0.000431 ***
## PTRATIO      -1.166270   0.123647  -9.432  &lt; 2e-16 ***
## B             0.010201   0.002743   3.719 0.000223 ***
## LSTAT        -0.533222   0.048701 -10.949  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.871 on 497 degrees of freedom
## Multiple R-squared:  0.724,  Adjusted R-squared:  0.7195 
## F-statistic: 162.9 on 8 and 497 DF,  p-value: &lt; 2.2e-16</code></pre>
<div id="model-diagmostic" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Model diagmostic</h3>
<p>To check that we have a linear relationship between the numerical explanatory variables and the response variable, we create a scatter plot with the variable and the residuals</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df2 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> df<span class="op">$</span>MEDV, <span class="dt">residuals =</span> model2_mlr_df<span class="op">$</span>residuals, <span class="dt">fitted =</span> model2_mlr_df<span class="op">$</span>fitted.values)
<span class="kw">ggplot</span>(df2, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> residuals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_jitter</span>()</code></pre></div>
<p><img src="machinelearningwithR_files/figure-html/linreg13-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(df2, <span class="kw">aes</span>(<span class="dt">x =</span> fitted, <span class="dt">y =</span> residuals)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_jitter</span>()</code></pre></div>
<p><img src="machinelearningwithR_files/figure-html/linreg14-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggfortify)
<span class="kw">autoplot</span>(model2_mlr_df)</code></pre></div>
<p><img src="machinelearningwithR_files/figure-html/linreg15-1.png" width="672" /></p>
</div>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">3.5</span> References</h2>
<ul>
<li>On the difference between the p-values of the F statistics and the R^2. <a href="https://analyticsdefined.com/interpreting-linear-regression-in-r/">Here</a><br />
</li>
<li>On the diagnotic plots and interpretation of results. <a href="http://data.library.virginia.edu/diagnostic-plots/">Here</a> and <a href="http://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/">here</a></li>
</ul>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Remember that the error term, <span class="math inline">\(\epsilon^i\)</span>, in the simple linear regression model is independent of x, and is normally distributed, with zero mean and constant variance.<a href="mlr.html#fnref1">â©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="testinference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/fderyckel/machinelearningwithr/edit/master/03-linear_regressions.Rmd",
"text": "Suggest edit to this page"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
