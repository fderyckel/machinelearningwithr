[
["index.html", "Machine Learning with R Chapter 1 Prerequisites", " Machine Learning with R François de Ryckel 2017-03-14 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). For now, you have to install the development versions of bookdown from Github: devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need to install XeLaTeX. "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2016) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["literature.html", "Chapter 3 Literature", " Chapter 3 Literature Here is a review of existing methods. "],
["trees-and-classification.html", "Chapter 4 Trees and Classification 4.1 Introduction 4.2 First example. 4.3 Second Example. 4.4 How does a tree decide where to split? 4.5 Third example. 4.6 References", " Chapter 4 Trees and Classification 4.1 Introduction Classification trees are non-parametric methods to recursively partition the data into more “pure” nodes, based on splitting rules. Logistic regression vs Decision trees. It is dependent on the type of problem you are solving. Let’s look at some key factors which will help you to decide which algorithm to use: If the relationship between dependent &amp; independent variable is well approximated by a linear model, linear regression will outperform tree based model. If there is a high non-linearity &amp; complex relationship between dependent &amp; independent variables, a tree model will outperform a classical regression method. If you need to build a model which is easy to explain to people, a decision tree model will always do better than a linear model. Decision tree models are even simpler to interpret than linear regression! The 2 main disadventages of Decision trees: Over fitting: Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning (discussed in detailed below). Not fit for continuous variables: While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories. Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes. 4.2 First example. Let’s do a CART on the iris dataset. This is the Hello World! of CART. library(rpart) library(rpart.plot) data(&quot;iris&quot;) str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... table(iris$Species) ## ## setosa versicolor virginica ## 50 50 50 tree &lt;- rpart(Species ~., data = iris, method = &quot;class&quot;) tree ## n= 150 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 150 100 setosa (0.33333333 0.33333333 0.33333333) ## 2) Petal.Length&lt; 2.45 50 0 setosa (1.00000000 0.00000000 0.00000000) * ## 3) Petal.Length&gt;=2.45 100 50 versicolor (0.00000000 0.50000000 0.50000000) ## 6) Petal.Width&lt; 1.75 54 5 versicolor (0.00000000 0.90740741 0.09259259) * ## 7) Petal.Width&gt;=1.75 46 1 virginica (0.00000000 0.02173913 0.97826087) * The method-argument can be switched according to the type of the response variable. It is class for categorial, anova for numerical, poisson for count data and `exp for survival data. Important Terminology related to Decision Trees Root Node: It represents entire population or sample and this further gets divided into two or more homogeneous sets. Splitting: It is a process of dividing a node into two or more sub-nodes. Decision Node: When a sub-node splits into further sub-nodes, then it is called decision node. Leaf/ Terminal Node: Nodes do not split is called Leaf or Terminal node. Pruning: When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting. Branch / Sub-Tree: A sub section of entire tree is called branch or sub-tree. Parent and Child Node: A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node. rpart.plot(tree) This is a model with a multi-class response. Each node shows the predicted class (setosa, versicolor, virginica), the predicted probability of each class, the percentage of observations in the node table(iris$Species, predict(tree, type = &quot;class&quot;)) ## ## setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 49 1 ## virginica 0 5 45 4.3 Second Example. Data set is the titanic. This is a model with a binary response. data(&quot;ptitanic&quot;) str(ptitanic) ## &#39;data.frame&#39;: 1309 obs. of 6 variables: ## $ pclass : Factor w/ 3 levels &quot;1st&quot;,&quot;2nd&quot;,&quot;3rd&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ survived: Factor w/ 2 levels &quot;died&quot;,&quot;survived&quot;: 2 2 1 1 1 2 2 1 2 1 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 1 2 1 2 1 2 1 2 ... ## $ age :Class &#39;labelled&#39; atomic [1:1309] 29 0.917 2 30 25 ... ## .. ..- attr(*, &quot;units&quot;)= chr &quot;Year&quot; ## .. ..- attr(*, &quot;label&quot;)= chr &quot;Age&quot; ## $ sibsp :Class &#39;labelled&#39; atomic [1:1309] 0 1 1 1 1 0 1 0 2 0 ... ## .. ..- attr(*, &quot;label&quot;)= chr &quot;Number of Siblings/Spouses Aboard&quot; ## $ parch :Class &#39;labelled&#39; atomic [1:1309] 0 2 2 2 2 0 0 0 0 0 ... ## .. ..- attr(*, &quot;label&quot;)= chr &quot;Number of Parents/Children Aboard&quot; ptitanic$age &lt;- as.numeric(ptitanic$age) ptitanic$sibsp &lt;- as.integer(ptitanic$sibsp) ptitanic$parch &lt;- as.integer(ptitanic$parch) Actually we can make the table more relevant. round(prop.table(table(ptitanic$sex, ptitanic$survived), 1), 2) ## ## died survived ## female 0.27 0.73 ## male 0.81 0.19 One can see here that the sum of the percentage add to 1 horizontally. If one want to make it vertically, we use 2. You can find the default limits by typing ?rpart.control. The first one we want to unleash is the cp parameter, this is the metric that stops splits that aren’t deemed important enough. The other one we want to open up is minsplit which governs how many passengers must sit in a bucket before even looking for a split. By putting a very low cp we are asking to have a very deep tree. The idea is that we prune it later. So in this first regression on ptitanic we’ll set a very low cp. library(rpart) library(rpart.plot) set.seed(123) tree &lt;- rpart(survived ~ ., data = ptitanic, cp=0.00001) rpart.plot(tree) Each node shows the predicted class (died or survived), the predicted probability of survival, the percentage of observations in the node. Let’s do a confusion matrix based on this tree. conf.matrix &lt;- round(prop.table(table(ptitanic$survived, predict(tree, type=&quot;class&quot;)), 2), 2) rownames(conf.matrix) &lt;- c(&quot;Actually died&quot;, &quot;Actually Survived&quot;) colnames(conf.matrix) &lt;- c(&quot;Predicted dead&quot;, &quot;Predicted Survived&quot;) conf.matrix ## ## Predicted dead Predicted Survived ## Actually died 0.83 0.16 ## Actually Survived 0.17 0.84 Let’s learn a bit more about trees. By using the name function, one can see all the object inherent to the tree function. A few intersting ones. The `$where component indicates to which leaf the different observations have been assigned. names(tree) ## [1] &quot;frame&quot; &quot;where&quot; &quot;call&quot; ## [4] &quot;terms&quot; &quot;cptable&quot; &quot;method&quot; ## [7] &quot;parms&quot; &quot;control&quot; &quot;functions&quot; ## [10] &quot;numresp&quot; &quot;splits&quot; &quot;csplit&quot; ## [13] &quot;variable.importance&quot; &quot;y&quot; &quot;ordered&quot; How to prune a tree? We want the cp value (with a simpler tree) that minimizes the xerror. So you need to find the lowest Cross-Validation Error. 2 ways to do this. Either the plotcp or the printcp functions. The plotcp is a visual representation of printcp function. The problem with reducing the `xerror is that the cross-validation error is a random quantity. There is no guarantee that if we were to fit the sequence of trees again using a different random seed that the same tree would minimize the cross-validation error. A more robust alternative to minimum cross-validation error is to use the one standard deviation rule: choose the smallest tree whose cross-validation error is within one standard error of the minimum. Depending on how we define this there are two possible choices. The first tree whose point estimate of the cross-validation error falls within the ± 1 xstd of the minimum. On the other hand the standard error lower limit of the tree of size three is within + 1 xstd of the minimum. Either of these is a reasonable choice, but insisting that the point estimate itself fall within the standard error limits is probably the more robust solution. As discussed earlier, the technique of setting constraint is a greedy-approach. In other words, it will check for the best split instantaneously and move forward until one of the specified stopping condition is reached. Let’s consider the following case when you’re driving: There are 2 lanes: A lane with cars moving at 80km/h A lane with trucks moving at 30km/h At this instant, you are a car in the fast lane and you have 2 choices: Take a left and overtake the other 2 cars quickly Keep moving in the present lane Lets analyze these choice. In the former choice, you’ll immediately overtake the car ahead and reach behind the truck and start moving at 30 km/h, looking for an opportunity to move back right. All cars originally behind you move ahead in the meanwhile. This would be the optimum choice if your objective is to maximize the distance covered in next say 10 seconds. In the later choice, you sale through at same speed, cross trucks and then overtake maybe depending on situation ahead. Greedy you! This is exactly the difference between normal decision tree &amp; pruning. A decision tree with constraints won’t see the truck ahead and adopt a greedy approach by taking a left. On the other hand if we use pruning, we in effect look at a few steps ahead and make a choice. So we know pruning is better. printcp(tree) ## ## Classification tree: ## rpart(formula = survived ~ ., data = ptitanic, cp = 1e-05) ## ## Variables actually used in tree construction: ## [1] age parch pclass sex sibsp ## ## Root node error: 500/1309 = 0.38197 ## ## n= 1309 ## ## CP nsplit rel error xerror xstd ## 1 0.4240000 0 1.000 1.000 0.035158 ## 2 0.0210000 1 0.576 0.576 0.029976 ## 3 0.0150000 3 0.534 0.570 0.029863 ## 4 0.0113333 5 0.504 0.566 0.029787 ## 5 0.0025714 9 0.458 0.530 0.029076 ## 6 0.0020000 16 0.440 0.530 0.029076 ## 7 0.0000100 18 0.436 0.534 0.029157 plotcp(tree) tree$cptable[which.min(tree$cptable[,&quot;xerror&quot;]),&quot;CP&quot;] ## [1] 0.002571429 See if we can prune slightly the tree bestcp &lt;- tree$cptable[which.min(tree$cptable[,&quot;xerror&quot;]),&quot;CP&quot;] tree.pruned &lt;- prune(tree, cp = bestcp) #this time we add a few arguments to add some mojo to our graphed tree. #Actually this will give us a very similar graphed tree as rattle (and we like that graph!) rpart.plot(tree.pruned, extra=104, box.palette=&quot;GnBu&quot;, branch.lty=3, shadow.col=&quot;gray&quot;, nn=TRUE) conf.matrix &lt;- round(prop.table(table(ptitanic$survived, predict(tree.pruned, type=&quot;class&quot;))), 2) rownames(conf.matrix) &lt;- c(&quot;Actually died&quot;, &quot;Actually Survived&quot;) colnames(conf.matrix) &lt;- c(&quot;Predicted dead&quot;, &quot;Predicted Survived&quot;) conf.matrix ## ## Predicted dead Predicted Survived ## Actually died 0.57 0.05 ## Actually Survived 0.13 0.25 Another way to check the output of the classifier is with a ROC (Receiver Operating Characteristics) Curve. This plots the true positive rate against the false positive rate, and gives us a visual feedback as to how well our model is performing. The package we will use for this is ROCR. library(ROCR) fit.pr = predict(tree.pruned, type=&quot;prob&quot;)[,2] fit.pred = prediction(fit.pr, ptitanic$survived) fit.perf = performance(fit.pred,&quot;tpr&quot;,&quot;fpr&quot;) plot(fit.perf,lwd=2,col=&quot;blue&quot;, main=&quot;ROC: Classification Trees on Titanic Dataset&quot;) abline(a=0,b=1) Ordinarily, using the confusion matrix for creating the ROC curve would give us a single point (as it is based off True positive rate vs false positive rate). What we do here is ask the prediction algorithm to give class probabilities to each observation, and then we plot the performance of the prediction using class probability as a cutoff. This gives us the “smooth” ROC curve. 4.4 How does a tree decide where to split? A bit more theory, before we go further. This part has been taken from this great tutorial. 4.5 Third example. The dataset I will be using for this third example is the “Adult” dataset hosted on UCI’s Machine Learning Repository. It contains approximately 32000 observations, with 15 variables. The dependent variable that in all cases we will be trying to predict is whether or not an “individual” has an income greater than $50,000 a year. Here is the set of variables contained in the data. age – The age of the individual type_employer – The type of employer the individual has. Whether they are government, military, private, an d so on. fnlwgt – The # of people the census takers believe that observation represents. We will be ignoring this variable education – The highest level of education achieved for that individual education_num – Highest level of education in numerical form marital – Marital status of the individual occupation – The occupation of the individual relationship – A bit more difficult to explain. Contains family relationship values like husband, father, and so on, but only contains one per observation. I’m not sure what this is supposed to represent race – descriptions of the individuals race. Black, White, Eskimo, and so on sex – Biological Sex capital_gain – Capital gains recorded capital_loss – Capital Losses recorded hr_per_week – Hours worked per week country – Country of origin for person income – Boolean Variable. Whether or not the person makes more than $50,000 per annum income. 4.6 References Trees with the rpart package Wholesale customers Data Set Origin of the data set of first example. Titanic: Getting Started With R - Part 3: Decision Trees. First understanding on how to read the graph of a tree. Classification and Regression Trees (CART) with rpart and rpart.plot. Got the Titanic example from there as well as a first understanding on pruning. Statistical Consulting Group. We learn here how to use the ROC curve. And we got out of it the adultdataset. A Complete Tutorial on Tree Based Modeling from Scratch (in R &amp; Python). This website is a real gems as always. Stephen Milborrow. rpart.plot: Plot rpart Models. An Enhanced Version of plot.rpart., 2016. R Package. It is important to cite the very generous people who dedicates so much of their time to offer us great tool. "],
["principal-component-analysis.html", "Chapter 5 Principal Component Analysis 5.1 PCA on an easy example. 5.2 Attempt of PCA on technical indicators. 5.3 Doing PCA and PCR with the PLS package 5.4 References.", " Chapter 5 Principal Component Analysis To create a predictive model based on regression we like to have as many relevant predictors as possible. The whole difficulty resides in finding relevant predictors. For predictors to be relevant, they should explain the variance of the dependent variable. Too many predictors (high dimensionality) and we take the risk of over-fitting. The intuition of Principal Component Analysis is to find new combination of variables which form larger variances. Why are larger variances important? This is a similar concept of entropy in information theory. Let’s say you have two variables. One of them (Var 1) forms N(1, 0.01) and the other (Var 2) forms N(1, 1). Which variable do you think has more information? Var 1 is always pretty much 1 whereas Var 2 can take a wider range of values, like 0 or 2. Thus, Var 2 has more chances to have various values than Var 1, which means Var 2’s entropy is larger than Var 1’s. Thus, we can say Var 2 contains more information than Var 1. PCA tries to find linear combination of the variables which contain much information by looking at the variance. This is why the standard deviation is one of the important metrics to determine the number of new variables in PCA. Another interesting aspect of the new variables derived by PCA is that all new variables are orthogonal. You can think that PCA is rotating and translating the data such that the first axis contains the most information, and the second has the second most information, and so forth. Principal Component Analysis (PCA) is a feature extraction methods that use orthogonal linear projections to capture the underlying variance of the data. When PCR compute the principle components is not looking at the response but only at the predictors (by looking for a linear combination of the predictors that has the highest variance). It makes the assumption that the linear combination of the predictors that has the highest variance is associated with the response. The algorithm when applied linearly transforms m-dimensional input space to n-dimensional (n &lt; m) output space, with the objective to minimize the amount of information/variance lost by discarding (m-n) dimensions. PCA allows us to discard the variables/features that have less variance. When choosing the principal component, we assume that the regression plane varies along the line and doesn’t vary in the other orthogonal direction. By choosing one component and not the other, we’re ignoring the second direction. PCR looks in the direction of variation of the predictors to find the places where the responses is most likely to vary. Some of the most notable advantages of performing PCA are the following: Dimensionality reduction Avoidance of multicollinearity between predictors. Variables are orthogonal, so including, say, PC9 in the model has no bearing on, say, PC3 Variables are ordered in terms of standard error. Thus, they also tend to be ordered in terms of statistical significance Overfitting mitigation The primary disadvantage is that this model is far more difficult to interpret than a regular logistic regression model With principal components regression, the new transformed variables (the principal components) are calculated in a totally unsupervised way: the response Y is not used to help determine the principal component directions). the response does not supervise the identification of the principal components. PCR just looks at the x variables The PCA method can dramatically improve estimation and insight in problems where multicollinearity is a large problem – as well as aid in detecting it. 5.1 PCA on an easy example. Let’s say we asked 16 participants four questions (on a 7 scale) about what they care about when choosing a new computer, and got the results like this. Price &lt;- c(6,7,6,5,7,6,5,6,3,1,2,5,2,3,1,2) Software &lt;- c(5,3,4,7,7,4,7,5,5,3,6,7,4,5,6,3) Aesthetics &lt;- c(3,2,4,1,5,2,2,4,6,7,6,7,5,6,5,7) Brand &lt;- c(4,2,5,3,5,3,1,4,7,5,7,6,6,5,5,7) buy_computer &lt;- data.frame(Price, Software, Aesthetics, Brand) Let’s go on with the PCA. princomp is part of the stats package. pca &lt;- princomp(buy_computer, cor = TRUE) names(pca) ## [1] &quot;sdev&quot; &quot;loadings&quot; &quot;center&quot; &quot;scale&quot; &quot;n.obs&quot; &quot;scores&quot; ## [7] &quot;call&quot; print(pca) ## Call: ## princomp(x = buy_computer, cor = TRUE) ## ## Standard deviations: ## Comp.1 Comp.2 Comp.3 Comp.4 ## 1.5589391 0.9804092 0.6816673 0.3792578 ## ## 4 variables and 16 observations. summary(pca, loadings = TRUE) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Standard deviation 1.5589391 0.9804092 0.6816673 0.37925777 ## Proportion of Variance 0.6075727 0.2403006 0.1161676 0.03595911 ## Cumulative Proportion 0.6075727 0.8478733 0.9640409 1.00000000 ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Price -0.523 0.848 ## Software -0.177 0.977 -0.120 ## Aesthetics 0.597 0.134 0.295 -0.734 ## Brand 0.583 0.167 0.423 0.674 OS &lt;- c(0,0,0,0,1,0,0,0,1,1,0,1,1,1,1,1) library(ggbiplot) g &lt;- ggbiplot(pca, obs.scale = 1, var.scale = 1, groups = as.character(OS), ellipse = TRUE, circle = TRUE) g &lt;- g + scale_color_discrete(name = &#39;&#39;) g &lt;- g + theme(legend.direction = &#39;horizontal&#39;, legend.position = &#39;top&#39;) print(g) The code below has been taken and modify from here. The idea is to plot each variables coefficients inside a unit circle to get insight on a possible interpretation for PCs. require(ggplot2) theta &lt;- seq(0,2*pi,length.out = 100) circle &lt;- data.frame(x = cos(theta), y = sin(theta)) p &lt;- ggplot(circle,aes(x,y)) + geom_path() loadings &lt;- data.frame(pca$rotation, .names = row.names(pca$rotation)) p + geom_text(data=loadings, mapping=aes(x = pca$loadings[,1], y = pca$loadings[,2], label = rownames(pca$loadings), colour = rownames(pca$loadings))) + coord_fixed(ratio=1) + labs(x = &quot;PC1&quot;, y = &quot;PC2&quot;) Remember that one of the disadventage of PCA is how difficult it is to interpret the model (ie. what does the PC1 is representing, what does PC2 is representing, etc.). The biplot graph help somehow to overcome that. In the above graph, one can see that Brandand Aesthetic explain most of the variance in the new predictor PC1 while Software explain most of the variance in the new predictor PC2. Once you have done the analysis with PCA, you may want to look into whether the new variables can predict some phenomena well. This is kinda like machine learning: Whether features can classify the data well. Let’s say you have asked the participants one more thing, which OS they are using (Windows or Mac) in your survey, and the results are like this. OS &lt;- c(0,0,0,0,1,0,0,0,1,1,0,1,1,1,1,1) # Let&#39;s test our model model1 &lt;- glm(OS ~ pca$scores[,1] + pca$scores[,2], family = binomial) summary(model1) ## ## Call: ## glm(formula = OS ~ pca$scores[, 1] + pca$scores[, 2], family = binomial) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4485 -0.4003 0.1258 0.5652 1.2814 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.2138 0.7993 -0.268 0.7891 ## pca$scores[, 1] 1.4744 0.6411 2.300 0.0215 * ## pca$scores[, 2] 0.7104 0.8940 0.795 0.4269 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 22.181 on 15 degrees of freedom ## Residual deviance: 11.338 on 13 degrees of freedom ## AIC: 17.338 ## ## Number of Fisher Scoring iterations: 5 Let’s see how well this model predicts the kind of OS. You can use fitted() function to see the prediction. fitted(model1) ## 1 2 3 4 5 6 ## 0.114201733 0.009372181 0.217716320 0.066009817 0.440016243 0.031640529 ## 7 8 9 10 11 12 ## 0.036189119 0.175766013 0.906761064 0.855587371 0.950088045 0.888272270 ## 13 14 15 16 ## 0.781098710 0.757499202 0.842557931 0.927223453 These values represent the probabilities of being 1. For example, we can expect 11% chance that Participant 1 is using OS 1 based on the variable derived by PCA. Thus, in this case, Participant 1 is more likely to be using OS 0, which agrees with the survey response. In this way, PCA can be used with regression models for calculating the probability of a phenomenon or making a prediction. I have tried to do the same with scaling the data using scale(x) and it changed absolutely nothing. 5.2 Attempt of PCA on technical indicators. For this purpose, we have taken a random stock, added a lots of variables and have one dependent variable. # Read the file library(readr) stock_data &lt;- read_csv(&quot;AugmentedStockData/CVX.csv&quot;) Now onto create our dependent variable and stipping down the data frame to just the columns that interest us, and only get rows and columns without NA. library(dplyr) binary = if_else(stock_data$ret3days[25:4150] &gt; 0.03, 1, 0) depend_var &lt;- stock_data[25:4150,8:34] The base R function prcomp() is used to perform PCA. PCA only works with normalized data. So we need to center the variable to have mean equals to zero. With parameter scale. = T, we normalize the variables to have standard deviation equals to 1. Normalized predictors have mean equals to zero and standard deviation equals to one. prin_comp &lt;- prcomp(depend_var, scale. = TRUE, center = TRUE) Let’s have a closer look at that ‘prcomp’ function. Center and scale refers to respective mean and standard deviation of the variables that are used for normalization prior to implementing PCA. names(prin_comp) ## [1] &quot;sdev&quot; &quot;rotation&quot; &quot;center&quot; &quot;scale&quot; &quot;x&quot; summary(prin_comp) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 ## Standard deviation 3.3007 2.5311 1.6647 1.58184 1.12216 0.89194 ## Proportion of Variance 0.4035 0.2373 0.1026 0.09268 0.04664 0.02947 ## Cumulative Proportion 0.4035 0.6408 0.7434 0.83609 0.88273 0.91220 ## PC7 PC8 PC9 PC10 PC11 PC12 ## Standard deviation 0.80601 0.63478 0.59745 0.53196 0.42057 0.39748 ## Proportion of Variance 0.02406 0.01492 0.01322 0.01048 0.00655 0.00585 ## Cumulative Proportion 0.93626 0.95118 0.96440 0.97488 0.98143 0.98728 ## PC13 PC14 PC15 PC16 PC17 PC18 ## Standard deviation 0.36869 0.33376 0.18342 0.17136 0.14502 0.08488 ## Proportion of Variance 0.00503 0.00413 0.00125 0.00109 0.00078 0.00027 ## Cumulative Proportion 0.99232 0.99644 0.99769 0.99878 0.99956 0.99982 ## PC19 PC20 PC21 PC22 PC23 PC24 ## Standard deviation 0.05953 0.02347 0.01988 0.01569 0.003083 0.002066 ## Proportion of Variance 0.00013 0.00002 0.00001 0.00001 0.000000 0.000000 ## Cumulative Proportion 0.99996 0.99998 0.99999 1.00000 1.000000 1.000000 ## PC25 PC26 PC27 ## Standard deviation 2.065e-15 1.629e-15 1.194e-15 ## Proportion of Variance 0.000e+00 0.000e+00 0.000e+00 ## Cumulative Proportion 1.000e+00 1.000e+00 1.000e+00 #outputs the mean of variables prin_comp$center ## wma3 wma5 wma7 wma9 wma11 ## 2.621433e-05 7.849430e-05 1.321253e-04 1.852866e-04 2.367136e-04 ## rsi_3val rsi_3dir rsi_5val rsi_5dir rsi_7val ## 5.249290e-01 2.259665e-01 5.219632e-01 5.112341e-02 5.198979e-01 ## rsi_7dir rsi_9val rsi_9dir rsi_11val rsi_11dir ## 2.280991e-02 5.183530e-01 1.295840e-02 5.171323e-01 8.388109e-03 ## 11arup 11ardow 11arosci 19arup 19ardow ## 5.318600e-01 4.556251e-01 7.623496e-02 5.355895e-01 4.540398e-01 ## 19arosci 23arup 23ardow 23arosci ave_vol3days ## 8.154961e-02 5.418344e-01 4.505785e-01 9.125587e-02 1.393280e-03 ## ave_vol5days ave_vol7days ## 3.236529e-03 4.388667e-03 #outputs the standard deviation of variables prin_comp$scale ## wma3 wma5 wma7 wma9 wma11 ## 0.01014433 0.01488609 0.01842084 0.02129883 0.02378999 ## rsi_3val rsi_3dir rsi_5val rsi_5dir rsi_7val ## 0.25196379 1.55382629 0.19466505 0.40810579 0.16395437 ## rsi_7dir rsi_9val rsi_9dir rsi_11val rsi_11dir ## 0.23886699 0.14412998 0.17015563 0.13002831 0.13287362 ## 11arup 11ardow 11arosci 19arup 19ardow ## 0.37456507 0.36908431 0.65809034 0.36982907 0.36010315 ## 19arosci 23arup 23ardow 23arosci ave_vol3days ## 0.64812635 0.36425219 0.35741487 0.63505784 0.18659298 ## ave_vol5days ave_vol7days ## 0.22564569 0.24793982 The rotation measure provides the principal component loading. Each column of rotation matrix contains the principal component loading vector. This is the most important measure we should be interested in. #because it can be a huge matrix, let&#39;s only check the first few rows and columns. prin_comp$rotation[1:5, 1:5] ## PC1 PC2 PC3 PC4 PC5 ## wma3 0.1788016 0.2543854 -0.00981180 -0.08087736 0.2431686 ## wma5 0.2086344 0.2209183 0.02753808 -0.18315739 0.2465858 ## wma7 0.2252186 0.1839885 0.04507260 -0.23309018 0.2168579 ## wma9 0.2360512 0.1499467 0.05369872 -0.25316224 0.1770487 ## wma11 0.2437448 0.1198431 0.05604151 -0.25599437 0.1376221 Let’s plot the resultant principal components. The prcomp() function also provides the facility to compute standard deviation of each principal component. sdev refers to the standard deviation of principal components. #compute standard deviation of each principal component std_dev &lt;- prin_comp$sdev #compute variance pr_var &lt;- std_dev^2 #check variance of first 10 components pr_var[1:10] ## [1] 10.8943784 6.4065586 2.7713407 2.5022255 1.2592331 0.7955608 ## [7] 0.6496465 0.4029457 0.3569417 0.2829776 We aim to find the components which explain the maximum variance. This is because, we want to retain as much information as possible using these components. So, higher is the explained variance, higher will be the information contained in those components. To compute the proportion of variance explained by each component, we simply divide the variance by sum of total variance. This results in: #proportion of variance explained prop_varex &lt;- pr_var/sum(pr_var) prop_varex[1:20] ## [1] 4.034955e-01 2.372799e-01 1.026422e-01 9.267502e-02 4.663826e-02 ## [6] 2.946522e-02 2.406098e-02 1.492391e-02 1.322006e-02 1.048065e-02 ## [11] 6.550995e-03 5.851611e-03 5.034607e-03 4.125722e-03 1.246026e-03 ## [16] 1.087586e-03 7.789197e-04 2.668137e-04 1.312590e-04 2.039572e-05 This shows that first principal component explains 41.7% variance. Second component explains 23.8% variance. Third component explains 10.4% variance and so on. So, how do we decide how many components should we select for modeling stage ? The answer to this question is provided by a scree plot. A scree plot is used to access components or factors which explains the most of variability in the data. It represents values in descending order. #scree plot plot(prop_varex, xlab = &quot;Principal Component&quot;, ylab = &quot;Proportion of Variance Explained&quot;, type = &quot;b&quot;) Or we can do a cumulative scree plot #cumulative scree plot plot(cumsum(prop_varex), xlab = &quot;Principal Component&quot;, ylab = &quot;Cumulative Proportion of Variance Explained&quot;, type = &quot;b&quot;) Hence in this case the first 6 Principal Components explain over 90% of the variance of the data. That is we can use these first 6 PC as predictor in our next model. Let’s apply this now on a logisitc regression model. For this, we need to create our binary dependent variable. So we’ll put a 1 for every ret3days &gt; 3%, 0 otherwise. mydata &lt;- cbind(binary, prin_comp$x) mydata &lt;- as.data.frame(mydata) model1 &lt;- glm(binary ~ PC1 + PC2 + PC5 + PC6 + PC7, data = mydata, family=binomial) summary(model1) ## ## Call: ## glm(formula = binary ~ PC1 + PC2 + PC5 + PC6 + PC7, family = binomial, ## data = mydata) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5159 -0.5197 -0.4511 -0.3740 2.4644 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.15924 0.05268 -40.989 &lt; 2e-16 *** ## PC1 -0.11636 0.01514 -7.685 1.53e-14 *** ## PC2 0.03253 0.01815 1.793 0.0731 . ## PC5 -0.05541 0.04094 -1.353 0.1760 ## PC6 -0.08353 0.05546 -1.506 0.1320 ## PC7 -0.04292 0.05665 -0.758 0.4486 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2851.7 on 4125 degrees of freedom ## Residual deviance: 2784.3 on 4120 degrees of freedom ## AIC: 2796.3 ## ## Number of Fisher Scoring iterations: 5 mydata &lt;- cbind(binary, prin_comp$x) mydata &lt;- as.data.frame(mydata) checkit &lt;- fitted(model1) checkit &lt;- cbind(binary, checkit) checkit &lt;- as.data.frame(checkit) head(checkit %&gt;% filter(binary == 1), 20) ## binary checkit ## 1 1 0.18260120 ## 2 1 0.18252003 ## 3 1 0.21606180 ## 4 1 0.21251118 ## 5 1 0.11869114 ## 6 1 0.14243678 ## 7 1 0.15250266 ## 8 1 0.08137055 ## 9 1 0.09081918 ## 10 1 0.07178808 ## 11 1 0.08675686 ## 12 1 0.08601504 ## 13 1 0.05538413 ## 14 1 0.13325771 ## 15 1 0.11428528 ## 16 1 0.13216528 ## 17 1 0.08095967 ## 18 1 0.07966423 ## 19 1 0.09559465 ## 20 1 0.16822463 head(checkit %&gt;% filter(checkit &gt; 0.2), 20) ## binary checkit ## 1 0 0.2199392 ## 2 1 0.2160618 ## 3 1 0.2125112 ## 4 0 0.2136409 ## 5 1 0.2187997 ## 6 1 0.2116685 ## 7 0 0.2070526 ## 8 0 0.2038604 ## 9 0 0.2097841 ## 10 0 0.2034205 ## 11 0 0.2187273 ## 12 1 0.2275500 ## 13 1 0.2281150 ## 14 0 0.2183669 ## 15 0 0.2060439 ## 16 0 0.2201554 ## 17 0 0.2692442 ## 18 1 0.2772148 ## 19 1 0.2574167 ## 20 1 0.2005783 Really not very successful model. 5.3 Doing PCA and PCR with the PLS package Same as before we can not have NA data in our set. library(pls) depend_var &lt;- stock_data[25:4150,8:35] pcr_model &lt;- pcr(ret3days~., data = depend_var, scale = TRUE, validation = &quot;CV&quot;) In oder to print out the results, simply use the summary function as below summary(pcr_model) ## Data: X dimension: 4126 27 ## Y dimension: 4126 1 ## Fit method: svdpc ## Number of components considered: 27 ## ## VALIDATION: RMSEP ## Cross-validated using 10 random segments. ## (Intercept) 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps ## CV 0.02651 0.0265 0.02651 0.02652 0.02653 0.02646 0.02646 ## adjCV 0.02651 0.0265 0.02651 0.02652 0.02653 0.02646 0.02645 ## 7 comps 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps ## CV 0.02649 0.02648 0.02646 0.02647 0.02647 0.02647 0.02647 ## adjCV 0.02648 0.02648 0.02645 0.02646 0.02646 0.02646 0.02646 ## 14 comps 15 comps 16 comps 17 comps 18 comps 19 comps ## CV 0.02647 0.02649 0.02648 0.02651 0.02651 0.02652 ## adjCV 0.02646 0.02648 0.02647 0.02650 0.02650 0.02651 ## 20 comps 21 comps 22 comps 23 comps 24 comps 25 comps ## CV 0.02653 0.02653 0.02654 0.02652 0.02651 0.02650 ## adjCV 0.02651 0.02652 0.02652 0.02650 0.02650 0.02646 ## 26 comps 27 comps ## CV 0.02655 0.02660 ## adjCV 0.02645 0.02648 ## ## TRAINING: % variance explained ## 1 comps 2 comps 3 comps 4 comps 5 comps 6 comps 7 comps ## X 40.3495 64.0775 74.3418 83.609 88.2731 91.22 93.626 ## ret3days 0.2797 0.3122 0.3381 0.359 0.9058 1.09 1.103 ## 8 comps 9 comps 10 comps 11 comps 12 comps 13 comps ## X 95.118 96.440 97.488 98.14 98.728 99.232 ## ret3days 1.122 1.521 1.523 1.56 1.685 1.738 ## 14 comps 15 comps 16 comps 17 comps 18 comps 19 comps ## X 99.644 99.769 99.878 99.956 99.982 99.996 ## ret3days 1.745 1.746 1.796 1.797 1.847 1.865 ## 20 comps 21 comps 22 comps 23 comps 24 comps 25 comps ## X 100.0 99.999 100.000 100.000 100.000 100.000 ## ret3days 1.9 1.997 2.017 2.166 2.215 2.268 ## 26 comps 27 comps ## X 100.000 100.000 ## ret3days 2.594 2.632 As you can see, two main results are printed, namely the validation error and the cumulative percentage of variance explained using n components. The cross validation results are computed for each number of components used so that you can easily check the score with a particular number of components without trying each combination on your own. The pls package provides also a set of methods to plot the results of PCR. For example you can plot the results of cross validation using the validationplot function. By default, the pcr function computes the root mean squared error and the validationplot function plots this statistic, however you can choose to plot the usual mean squared error or the R2 by setting the val.type argument equal to “MSEP” or “R2” respectively # Plot the root mean squared error validationplot(pcr_model) What you would like to see is a low cross validation error with a lower number of components than the number of variables in your dataset. If this is not the case or if the smalles cross validation error occurs with a number of components close to the number of variables in the original data, then no dimensionality reduction occurs. In the example above, it looks like 3 components are enough to explain more than 90% of the variability in the data. Now you can try to use PCR on a traning-test set and evaluate its performance using, for example, using only 6 components # Train-test split train &lt;- stock_data[25:3000,8:35] y_test &lt;- stock_data[3001:4150,35] test &lt;- stock_data[3001:4150,8:34] pcr_model &lt;- pcr(ret3days~., data = train,scale =TRUE, validation = &quot;CV&quot;) pcr_pred &lt;- predict(pcr_model, test, ncomp = 6) mean((pcr_pred - y_test)^2) ## [1] 0.0005600123 5.4 References. Here are the articles I have consulted for this research. Principal Component Analysis (PCA) Principal Component Analysis using R Computing and visualizing PCA in R This is where we learned about the `ggbiplot Practical Guide to Principal Component Analysis (PCA) in R &amp; Python Performing Principal Components Regression (PCR) in R Data Mining - Principal Component (Analysis|Regression) (PCA) PRINCIPAL COMPONENT ANALYSIS IN R A really nice explanation on the difference between the main packages doing PCA such as svd, princompand prcomp. In R there are two general methods to perform PCA without any missing values: The spectral decomposition method of analysis examines the covariances and correlations between variables, whereas the singular value decomposition method looks at the covariances and correlations among the samples. While both methods can easily be performed within R, the singular value decomposition method is the preferred analysis for numerical accuracy. Although principal component analysis assumes multivariate normality, this is not a very strict assumption, especially when the procedure is used for data reduction or exploratory purposes. Undoubtedly, the correlation and covariance matrices are better measures of similarity if the data is normal, and yet, PCA is often unaffected by mild violations. However, if the new components are to be used in further analyses, such as regression analysis, normality of the data might be more important. "],
["case-study-mushrooms-classification.html", "Chapter 6 Case Study - Mushrooms Classification 6.1 Import the data 6.2 Tidy the data 6.3 Understand the data 6.4 Communication 6.5 Example one 6.6 Example two", " Chapter 6 Case Study - Mushrooms Classification This chapter demonstrates how to classify muhsrooms as edible or not. It also answera the question: what are the main characteristics of an edible mushroom? This blog post gave us first the idea and we followed most of it. We also noticed that Kaggle has put online the same data set and classification exercise. We have taken inspiration from some posts here and here The data set is available on the Machine Learning Repository of the UC Irvine website. 6.1 Import the data The data set is given to us in a rough form and quite a bit of editing is necessary. # Load the data - we downloaded the data from the website and saved it into a .csv file library(readr) library(dplyr) mushroom &lt;- read_csv(&quot;dataset/Mushroom.csv&quot;, col_names = FALSE) glimpse(mushroom) ## Observations: 8,124 ## Variables: 23 ## $ X1 &lt;chr&gt; &quot;p&quot;, &quot;e&quot;, &quot;e&quot;, &quot;p&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;p&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;... ## $ X2 &lt;chr&gt; &quot;x&quot;, &quot;x&quot;, &quot;b&quot;, &quot;x&quot;, &quot;x&quot;, &quot;x&quot;, &quot;b&quot;, &quot;b&quot;, &quot;x&quot;, &quot;b&quot;, &quot;x&quot;, &quot;x&quot;... ## $ X3 &lt;chr&gt; &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;y&quot;, &quot;s&quot;, &quot;y&quot;, &quot;s&quot;, &quot;y&quot;, &quot;y&quot;, &quot;s&quot;, &quot;y&quot;, &quot;y&quot;... ## $ X4 &lt;chr&gt; &quot;n&quot;, &quot;y&quot;, &quot;w&quot;, &quot;w&quot;, &quot;g&quot;, &quot;y&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;y&quot;, &quot;y&quot;, &quot;y&quot;... ## $ X5 &lt;chr&gt; &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;f&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;, &quot;t&quot;... ## $ X6 &lt;chr&gt; &quot;p&quot;, &quot;a&quot;, &quot;l&quot;, &quot;p&quot;, &quot;n&quot;, &quot;a&quot;, &quot;a&quot;, &quot;l&quot;, &quot;p&quot;, &quot;a&quot;, &quot;l&quot;, &quot;a&quot;... ## $ X7 &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;... ## $ X8 &lt;chr&gt; &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;w&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;... ## $ X9 &lt;chr&gt; &quot;n&quot;, &quot;b&quot;, &quot;b&quot;, &quot;n&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;n&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;... ## $ X10 &lt;chr&gt; &quot;k&quot;, &quot;k&quot;, &quot;n&quot;, &quot;n&quot;, &quot;k&quot;, &quot;n&quot;, &quot;g&quot;, &quot;n&quot;, &quot;p&quot;, &quot;g&quot;, &quot;g&quot;, &quot;n&quot;... ## $ X11 &lt;chr&gt; &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;t&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;, &quot;e&quot;... ## $ X12 &lt;chr&gt; &quot;e&quot;, &quot;c&quot;, &quot;c&quot;, &quot;e&quot;, &quot;e&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;e&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;... ## $ X13 &lt;chr&gt; &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;... ## $ X14 &lt;chr&gt; &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;, &quot;s&quot;... ## $ X15 &lt;chr&gt; &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;... ## $ X16 &lt;chr&gt; &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;... ## $ X17 &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;... ## $ X18 &lt;chr&gt; &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;... ## $ X19 &lt;chr&gt; &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;, &quot;o&quot;... ## $ X20 &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;e&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;... ## $ X21 &lt;chr&gt; &quot;k&quot;, &quot;n&quot;, &quot;n&quot;, &quot;k&quot;, &quot;n&quot;, &quot;k&quot;, &quot;k&quot;, &quot;n&quot;, &quot;k&quot;, &quot;k&quot;, &quot;n&quot;, &quot;k&quot;... ## $ X22 &lt;chr&gt; &quot;s&quot;, &quot;n&quot;, &quot;n&quot;, &quot;s&quot;, &quot;a&quot;, &quot;n&quot;, &quot;n&quot;, &quot;s&quot;, &quot;v&quot;, &quot;s&quot;, &quot;n&quot;, &quot;s&quot;... ## $ X23 &lt;chr&gt; &quot;u&quot;, &quot;g&quot;, &quot;m&quot;, &quot;u&quot;, &quot;g&quot;, &quot;g&quot;, &quot;m&quot;, &quot;m&quot;, &quot;g&quot;, &quot;m&quot;, &quot;g&quot;, &quot;m&quot;... Basically we have 8124 mushrooms in the dataset. And each observation consists of 23 variables. As it stands, the data frame doesn’t look very meaningfull. We have to go back to the source to bring meaning to each of the variables and to the various levels of the categorical variables. 6.2 Tidy the data This is the least fun part of the workflow. We’ll start by giving names to each of the variables, then we specify the category for each variable. It is not necessary to do so but it does add meaning to what we do. # Rename the variables colnames(mushroom) &lt;- c(&quot;edibility&quot;, &quot;cap_shape&quot;, &quot;cap_surface&quot;, &quot;cap_color&quot;, &quot;bruises&quot;, &quot;odor&quot;, &quot;gill_attachement&quot;, &quot;gill_spacing&quot;, &quot;gill_size&quot;, &quot;gill_color&quot;, &quot;stalk_shape&quot;, &quot;stalk_root&quot;, &quot;stalk_surface_above_ring&quot;, &quot;stalk_surface_below_ring&quot;, &quot;stalk_color_above_ring&quot;, &quot;stalk_color_below_ring&quot;, &quot;veil_type&quot;, &quot;veil_color&quot;, &quot;ring_number&quot;, &quot;ring_type&quot;, &quot;spore_print_color&quot;, &quot;population&quot;, &quot;habitat&quot;) # Defining the levels for the categorical variables ## We make each variable as a factor library(purrr) mushroom &lt;- mushroom %&gt;% map_df(function(.x) as.factor(.x)) ## We redefine each of the category for each of the variables levels(mushroom$edibility) &lt;- c(&quot;edible&quot;, &quot;poisonous&quot;) levels(mushroom$cap_shape) &lt;- c(&quot;bell&quot;, &quot;conical&quot;, &quot;flat&quot;, &quot;knobbed&quot;, &quot;sunken&quot;, &quot;convex&quot;) levels(mushroom$cap_color) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$cap_surface) &lt;- c(&quot;fibrous&quot;, &quot;grooves&quot;, &quot;scaly&quot;, &quot;smooth&quot;) levels(mushroom$bruises) &lt;- c(&quot;no&quot;, &quot;yes&quot;) levels(mushroom$odor) &lt;- c(&quot;almond&quot;, &quot;creosote&quot;, &quot;foul&quot;, &quot;anise&quot;, &quot;musty&quot;, &quot;none&quot;, &quot;pungent&quot;, &quot;spicy&quot;, &quot;fishy&quot;) levels(mushroom$gill_attachement) &lt;- c(&quot;attached&quot;, &quot;free&quot;) levels(mushroom$gill_spacing) &lt;- c(&quot;close&quot;, &quot;crowded&quot;) levels(mushroom$gill_size) &lt;- c(&quot;broad&quot;, &quot;narrow&quot;) levels(mushroom$gill_color) &lt;- c(&quot;buff&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;chocolate&quot;, &quot;black&quot;, &quot;brown&quot;, &quot;orange&quot;, &quot;pink&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$stalk_shape) &lt;- c(&quot;enlarging&quot;, &quot;tapering&quot;) levels(mushroom$stalk_root) &lt;- c(&quot;missing&quot;, &quot;bulbous&quot;, &quot;club&quot;, &quot;equal&quot;, &quot;rooted&quot;) levels(mushroom$stalk_surface_above_ring) &lt;- c(&quot;fibrous&quot;, &quot;silky&quot;, &quot;smooth&quot;, &quot;scaly&quot;) levels(mushroom$stalk_surface_below_ring) &lt;- c(&quot;fibrous&quot;, &quot;silky&quot;, &quot;smooth&quot;, &quot;scaly&quot;) levels(mushroom$stalk_color_above_ring) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$stalk_color_below_ring) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$veil_type) &lt;- &quot;partial&quot; levels(mushroom$veil_color) &lt;- c(&quot;brown&quot;, &quot;orange&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$ring_number) &lt;- c(&quot;none&quot;, &quot;one&quot;, &quot;two&quot;) levels(mushroom$ring_type) &lt;- c(&quot;evanescent&quot;, &quot;flaring&quot;, &quot;large&quot;, &quot;none&quot;, &quot;pendant&quot;) levels(mushroom$spore_print_color) &lt;- c(&quot;buff&quot;, &quot;chocolate&quot;, &quot;black&quot;, &quot;brown&quot;, &quot;orange&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;) levels(mushroom$population) &lt;- c(&quot;abundant&quot;, &quot;clustered&quot;, &quot;numerous&quot;, &quot;scattered&quot;, &quot;several&quot;, &quot;solitary&quot;) levels(mushroom$habitat) &lt;- c(&quot;wood&quot;, &quot;grasses&quot;, &quot;leaves&quot;, &quot;meadows&quot;, &quot;paths&quot;, &quot;urban&quot;, &quot;waste&quot;) glimpse(mushroom) ## Observations: 8,124 ## Variables: 23 ## $ edibility &lt;fctr&gt; poisonous, edible, edible, poisonous... ## $ cap_shape &lt;fctr&gt; convex, convex, bell, convex, convex... ## $ cap_surface &lt;fctr&gt; scaly, scaly, scaly, smooth, scaly, ... ## $ cap_color &lt;fctr&gt; brown, yellow, white, white, gray, y... ## $ bruises &lt;fctr&gt; yes, yes, yes, yes, no, yes, yes, ye... ## $ odor &lt;fctr&gt; pungent, almond, anise, pungent, non... ## $ gill_attachement &lt;fctr&gt; free, free, free, free, free, free, ... ## $ gill_spacing &lt;fctr&gt; close, close, close, close, crowded,... ## $ gill_size &lt;fctr&gt; narrow, broad, broad, narrow, broad,... ## $ gill_color &lt;fctr&gt; black, black, brown, brown, black, b... ## $ stalk_shape &lt;fctr&gt; enlarging, enlarging, enlarging, enl... ## $ stalk_root &lt;fctr&gt; equal, club, club, equal, equal, clu... ## $ stalk_surface_above_ring &lt;fctr&gt; smooth, smooth, smooth, smooth, smoo... ## $ stalk_surface_below_ring &lt;fctr&gt; smooth, smooth, smooth, smooth, smoo... ## $ stalk_color_above_ring &lt;fctr&gt; purple, purple, purple, purple, purp... ## $ stalk_color_below_ring &lt;fctr&gt; purple, purple, purple, purple, purp... ## $ veil_type &lt;fctr&gt; partial, partial, partial, partial, ... ## $ veil_color &lt;fctr&gt; white, white, white, white, white, w... ## $ ring_number &lt;fctr&gt; one, one, one, one, one, one, one, o... ## $ ring_type &lt;fctr&gt; pendant, pendant, pendant, pendant, ... ## $ spore_print_color &lt;fctr&gt; black, brown, brown, black, brown, b... ## $ population &lt;fctr&gt; scattered, numerous, numerous, scatt... ## $ habitat &lt;fctr&gt; urban, grasses, meadows, urban, gras... As each variables is categorical, let’s see how many categories are we speaking about? library(tibble) number_class &lt;- function(x){ x &lt;- length(levels(x)) } x &lt;- mushroom %&gt;% map_dbl(function(.x) number_class(.x)) %&gt;% as_tibble() %&gt;% rownames_to_column() %&gt;% arrange(desc(value)) colnames(x) &lt;- c(&quot;Variable name&quot;, &quot;Number of levels&quot;) print(x) ## # A tibble: 23 × 2 ## `Variable name` `Number of levels` ## &lt;chr&gt; &lt;dbl&gt; ## 1 gill_color 12 ## 2 cap_color 10 ## 3 stalk_color_above_ring 10 ## 4 stalk_color_below_ring 10 ## 5 odor 9 ## 6 spore_print_color 9 ## 7 habitat 7 ## 8 cap_shape 6 ## 9 population 6 ## 10 stalk_root 5 ## # ... with 13 more rows 6.3 Understand the data This is the circular phase of our dealing with data. This is where each of the transforming, visualizing and modeling stage reinforce each other to create a better understanding. 6.3.1 A. Transform the data We noticed from the previous section an issue with the veil_type variable. It has only one factor. So basically, it does not bring any information. Furthermore, factor variable with only one level do create issues later on at the modeling stage. R will throw out an error for the categorical variable that has only one level. So let’s take away that column. mushroom &lt;- mushroom %&gt;% select(- veil_type) Do we have any missing data? Most ML algorithms won’t work if we have missing data. map_dbl(mushroom, function(.x) {sum(is.na(.x))}) ## edibility cap_shape cap_surface ## 0 0 0 ## cap_color bruises odor ## 0 0 0 ## gill_attachement gill_spacing gill_size ## 0 0 0 ## gill_color stalk_shape stalk_root ## 0 0 0 ## stalk_surface_above_ring stalk_surface_below_ring stalk_color_above_ring ## 0 0 0 ## stalk_color_below_ring veil_color ring_number ## 0 0 0 ## ring_type spore_print_color population ## 0 0 0 ## habitat ## 0 Lucky us! We have no missing data. 6.3.2 A. Visualize the data This is one of the most important step in the DS process. This stage can gives us unexpected insights and often allows us to ask the right questions. library(ggplot2) ggplot(mushroom, aes(x = cap_surface, y = cap_color, col = edibility)) + geom_jitter(alpha = 0.5) + scale_color_manual(breaks = c(&quot;edible&quot;, &quot;poisonous&quot;), values = c(&quot;green&quot;, &quot;red&quot;)) If we want to stay safe, better bet on fibrous surface. Stay especially away from smooth surface, except if they are purple or green. ggplot(mushroom, aes(x = cap_shape, y = cap_color, col = edibility)) + geom_jitter(alpha = 0.5) + scale_color_manual(breaks = c(&quot;edible&quot;, &quot;poisonous&quot;), values = c(&quot;green&quot;, &quot;red&quot;)) Again, in case one don’t know about mushroom, it is better to stay away from all shapes except maybe for bell shape mushrooms. ggplot(mushroom, aes(x = gill_color, y = cap_color, col = edibility)) + geom_jitter(alpha = 0.5) + scale_color_manual(breaks = c(&quot;edible&quot;, &quot;poisonous&quot;), values = c(&quot;green&quot;, &quot;red&quot;)) ggplot(mushroom, aes(x = edibility, y = odor, col = edibility)) + geom_jitter(alpha = 0.5) + scale_color_manual(breaks = c(&quot;edible&quot;, &quot;poisonous&quot;), values = c(&quot;green&quot;, &quot;red&quot;)) Odor is defintely quite an informative predictor. Basically, if it smells fishy, spicy or pungent just stay away. If it smells like anise or almond you can go ahead. If it doesn’t smell anything, you have better chance that it is edible than not. TO DO: put a comment on what we see TO DO: put a mosaic graph 6.3.3 A. Modeling At this stage, we should have gathered enough information and insights on our data to choose appropriate modeling techniques. Before we go ahead, we need to split the data into a training and testing set set.seed(1810) mushsample &lt;- caret::createDataPartition(y = mushroom$edibility, times = 1, p = 0.8, list = FALSE) train_mushroom &lt;- mushroom[mushsample, ] test_mushroom &lt;- mushroom[-mushsample, ] We can check the quality of the splits in regards to our predicted (dependent) variable. round(prop.table(table(mushroom$edibility)), 2) ## ## edible poisonous ## 0.52 0.48 round(prop.table(table(train_mushroom$edibility)), 2) ## ## edible poisonous ## 0.52 0.48 round(prop.table(table(test_mushroom$edibility)), 2) ## ## edible poisonous ## 0.52 0.48 It seems like we have the right splits. 6.3.3.1 A. Use of Regression Tree As we have many categorical variables, regression tree is an ideal classification tools for such situation. We’ll use the rpart package. Let’s give it a try without any customization. library(rpart) library(rpart.plot) set.seed(1810) model_tree &lt;- rpart(edibility ~ ., data = train_mushroom, method = &quot;class&quot;) model_tree ## n= 6500 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 6500 3133 edible (0.51800000 0.48200000) ## 2) odor=almond,anise,none 3468 101 edible (0.97087659 0.02912341) ## 4) spore_print_color=buff,chocolate,black,brown,orange,purple,white,yellow 3408 41 edible (0.98796948 0.01203052) * ## 5) spore_print_color=green 60 0 poisonous (0.00000000 1.00000000) * ## 3) odor=creosote,foul,musty,pungent,spicy,fishy 3032 0 poisonous (0.00000000 1.00000000) * caret::confusionMatrix(data=predict(model_tree, type = &quot;class&quot;), reference = train_mushroom$edibility, positive=&quot;edible&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction edible poisonous ## edible 3367 41 ## poisonous 0 3092 ## ## Accuracy : 0.9937 ## 95% CI : (0.9915, 0.9955) ## No Information Rate : 0.518 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9874 ## Mcnemar&#39;s Test P-Value : 4.185e-10 ## ## Sensitivity : 1.0000 ## Specificity : 0.9869 ## Pos Pred Value : 0.9880 ## Neg Pred Value : 1.0000 ## Prevalence : 0.5180 ## Detection Rate : 0.5180 ## Detection Prevalence : 0.5243 ## Balanced Accuracy : 0.9935 ## ## &#39;Positive&#39; Class : edible ## We have quite an issue here. 40 mushrooms have been predicted as edible but were actually poisonous. That should not be happening. So we’ll set up a penalty for wrongly predicting a mushroom as edible when in reality it is poisonous. A mistake the other way is not as bad. At worst we miss on a good recipe! So let’s redo our tree with a penalty for wrongly predicting poisonous. To do this, we introduce a penalty matrix that we’ll use as a parameter in our rpart function. penalty_matrix &lt;- matrix(c(0, 1, 10, 0), byrow = TRUE, nrow = 2) model_tree_penalty &lt;- rpart(edibility ~ ., data = train_mushroom, method = &quot;class&quot;, parms = list(loss = penalty_matrix)) caret::confusionMatrix(data=predict(model_tree_penalty, type = &quot;class&quot;), reference = train_mushroom$edibility, positive=&quot;edible&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction edible poisonous ## edible 3367 0 ## poisonous 0 3133 ## ## Accuracy : 1 ## 95% CI : (0.9994, 1) ## No Information Rate : 0.518 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.000 ## Specificity : 1.000 ## Pos Pred Value : 1.000 ## Neg Pred Value : 1.000 ## Prevalence : 0.518 ## Detection Rate : 0.518 ## Detection Prevalence : 0.518 ## Balanced Accuracy : 1.000 ## ## &#39;Positive&#39; Class : edible ## So introducing a penalty did the job; it gave us a perfect prediction and saves us from a jounrey at the hospital. Another way to increase the accuracy of our tree model is to play on the cp parameter. We start to build a tree with a very low cp (that is we’ll have a deep tree). The idea is that we then prune it later. model_tree &lt;- rpart(edibility ~ ., data = train_mushroom, method = &quot;class&quot;, cp = 0.00001) To prune a tree, we first have to find the cp that gives the lowest xerror or cross-validation error. We can find the lowest xerror using either the printcp or plotcp function. printcp(model_tree) ## ## Classification tree: ## rpart(formula = edibility ~ ., data = train_mushroom, method = &quot;class&quot;, ## cp = 1e-05) ## ## Variables actually used in tree construction: ## [1] cap_surface habitat odor ## [4] spore_print_color stalk_color_below_ring stalk_root ## ## Root node error: 3133/6500 = 0.482 ## ## n= 6500 ## ## CP nsplit rel error xerror xstd ## 1 0.9677625 0 1.0000000 1.0000000 0.01285833 ## 2 0.0191510 1 0.0322375 0.0322375 0.00318273 ## 3 0.0063837 2 0.0130865 0.0130865 0.00203731 ## 4 0.0022343 3 0.0067028 0.0067028 0.00146032 ## 5 0.0011171 5 0.0022343 0.0022343 0.00084402 ## 6 0.0000100 7 0.0000000 0.0022343 0.00084402 We can see here that that the lowest xerror happen at the 5th split. plotcp(model_tree) model_tree$cptable[which.min(model_tree$cptable[, &quot;xerror&quot;]), &quot;CP&quot;] ## [1] 0.00111714 So now we can start pruning our tree with the cp that gives the lowest cross-validation error. bestcp &lt;- round(model_tree$cptable[which.min(model_tree$cptable[, &quot;xerror&quot;]), &quot;CP&quot;], 4) model_tree_pruned &lt;- prune(model_tree, cp = bestcp) Let’s have a quick look at the tree as it stands rpart.plot(model_tree_pruned, extra = 104, box.palette = &quot;GnBu&quot;, branch.lty = 3, shadow.col = &quot;gray&quot;, nn = TRUE) How does the model perform on the train data? #table(train_mushroom$edibility, predict(model_tree, type=&quot;class&quot;)) caret::confusionMatrix(data=predict(model_tree_pruned, type = &quot;class&quot;), reference = train_mushroom$edibility, positive=&quot;edible&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction edible poisonous ## edible 3367 0 ## poisonous 0 3133 ## ## Accuracy : 1 ## 95% CI : (0.9994, 1) ## No Information Rate : 0.518 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.000 ## Specificity : 1.000 ## Pos Pred Value : 1.000 ## Neg Pred Value : 1.000 ## Prevalence : 0.518 ## Detection Rate : 0.518 ## Detection Prevalence : 0.518 ## Balanced Accuracy : 1.000 ## ## &#39;Positive&#39; Class : edible ## It seems like we have a perfect accuracy on our training set. It is quite rare to have such perfect accuracy. Let’s check how it fares on the testing set. test_tree &lt;- predict(model_tree, newdata = test_mushroom) caret::confusionMatrix(data = predict(model_tree, newdata = test_mushroom, type = &quot;class&quot;), reference = test_mushroom$edibility, positive = &quot;edible&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction edible poisonous ## edible 841 0 ## poisonous 0 783 ## ## Accuracy : 1 ## 95% CI : (0.9977, 1) ## No Information Rate : 0.5179 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.0000 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 1.0000 ## Prevalence : 0.5179 ## Detection Rate : 0.5179 ## Detection Prevalence : 0.5179 ## Balanced Accuracy : 1.0000 ## ## &#39;Positive&#39; Class : edible ## Perfect prediction here as well. 6.3.3.2 A. Use of Random Forest We usually use random forest if a tree is not enough. In this case, as we have perfect prediction using a single tree, it is not really necessary to use a Random Forest algorithm. We just use for learning sake without tuning any of the parameters. library(randomForest) model_rf &lt;- randomForest(edibility ~ ., ntree = 50, data = train_mushroom) plot(model_rf) The default number of trees for the random forest is 500; we just use 50 here. As we can see on the plot, above 20 trees, the error isn’t decreasing anymore. And actually, the error seems to be 0 or almost 0. The next step can tell us this more accurately. print(model_rf) ## ## Call: ## randomForest(formula = edibility ~ ., data = train_mushroom, ntree = 50) ## Type of random forest: classification ## Number of trees: 50 ## No. of variables tried at each split: 4 ## ## OOB estimate of error rate: 0% ## Confusion matrix: ## edible poisonous class.error ## edible 3367 0 0 ## poisonous 0 3133 0 Altough it is not really necessary to this here as we have a perfect prediction, we can use the confusionMatrix function from the caret pacakge. caret::confusionMatrix(data = model_rf$predicted, reference = train_mushroom$edibility , positive = &quot;edible&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction edible poisonous ## edible 3367 0 ## poisonous 0 3133 ## ## Accuracy : 1 ## 95% CI : (0.9994, 1) ## No Information Rate : 0.518 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.000 ## Specificity : 1.000 ## Pos Pred Value : 1.000 ## Neg Pred Value : 1.000 ## Prevalence : 0.518 ## Detection Rate : 0.518 ## Detection Prevalence : 0.518 ## Balanced Accuracy : 1.000 ## ## &#39;Positive&#39; Class : edible ## If we want to look at the most important variable in terms of predicting edibility in our model, we can do that using the Mean Decreasing Gini varImpPlot(model_rf, sort = TRUE, n.var = 10, main = &quot;The 10 variables with the most predictive power&quot;) Another way to look at the predictible power of the variables is to use the importance extractor function. library(tibble) importance(model_rf) %&gt;% data.frame() %&gt;% rownames_to_column(var = &quot;Variable&quot;) %&gt;% arrange(desc(MeanDecreaseGini)) %&gt;% head(10) ## Variable MeanDecreaseGini ## 1 odor 1115.85522 ## 2 spore_print_color 477.71557 ## 3 gill_color 319.02467 ## 4 stalk_surface_above_ring 235.59574 ## 5 gill_size 194.56155 ## 6 stalk_surface_below_ring 172.26749 ## 7 stalk_root 132.26045 ## 8 ring_type 129.88445 ## 9 population 79.42030 ## 10 gill_spacing 63.42436 We could compare that with the important variables from the classification tree obtained above. model_tree_penalty$variable.importance %&gt;% as_tibble() %&gt;% rownames_to_column(var = &quot;variable&quot;) %&gt;% arrange(desc(value)) %&gt;% head(10) ## # A tibble: 10 × 2 ## variable value ## &lt;chr&gt; &lt;dbl&gt; ## 1 odor 848.00494 ## 2 spore_print_color 804.39831 ## 3 gill_color 503.71270 ## 4 stalk_surface_above_ring 501.28385 ## 5 stalk_surface_below_ring 453.92877 ## 6 ring_type 450.29286 ## 7 ring_number 170.56141 ## 8 stalk_root 117.78800 ## 9 habitat 98.22176 ## 10 stalk_color_below_ring 74.72602 Interestingly gill_size which is the 5th most important predictor in the random forest does not appear in the top 10 of our classification tree. Now we apply our model to our testing set. test_rf &lt;- predict(model_rf, newdata = test_mushroom) # Quick check on our prediction table(test_rf, test_mushroom$edibility) ## ## test_rf edible poisonous ## edible 841 0 ## poisonous 0 783 Perfect Prediction! 6.3.3.3 A. Use of SVM library(e1071) model_svm &lt;- svm(edibility ~. , data=train_mushroom, cost = 1000, gamma = 0.01) Check the prediction test_svm &lt;- predict(model_svm, newdata = test_mushroom) table(test_svm, test_mushroom$edibility) ## ## test_svm edible poisonous ## edible 841 0 ## poisonous 0 783 And perfect prediction again! 6.4 Communication With some fine tuning, a regression tree managed to predict accurately the edibility of mushroom. They were 2 parameters to look at: the cpand the penalty matrix. Random Forest and SVM achieved similar results out of the box. The regression tree approach has to be prefered as it is a lot easier to grasp the results from a tree than from a SVM algorithm. For sure I will take my little tree picture next time I go shrooming. That said, I will still only go with a good mycologist. We would love to hear from you. Give us feedback below. 6.5 Example one 6.6 Example two "],
["case-study-predicting-survicalship-on-the-titanic.html", "Chapter 7 Case Study - Predicting Survicalship on the Titanic 7.1 Import the data. 7.2 Tidy the data 7.3 Understand the data 7.4 A. Visualize with cabins. 7.5 B. Transform Dealing with missing data. 7.6 References.", " Chapter 7 Case Study - Predicting Survicalship on the Titanic This chapter demonstrates another example of classification with machine learning. Kaggle made this exercise quite popular. In this study, the training and test sets have already been defined, so we 7.1 Import the data. We have put our data into our google drive here and here. You can find them on Kaggle if need be. library(readr) library(dplyr) train_set &lt;- read_csv(&quot;~/Google Drive/Software/R projects/datasets/Kaggle_Titanic_train.csv&quot;) test_set &lt;- read_csv(&quot;~/Google Drive/Software/R projects/datasets/Kaggle_Titanic_test.csv&quot;) ## Let&#39;s bind both set of data for our exploratory analysis. mydata &lt;- bind_rows(train_set, test_set) ## Let&#39;s have a first glimpse to our data glimpse(mydata) ## Observations: 1,309 ## Variables: 12 ## $ PassengerId &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,... ## $ Survived &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,... ## $ Pclass &lt;int&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3,... ## $ Name &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bra... ## $ Sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;mal... ## $ Age &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, ... ## $ SibSp &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4,... ## $ Parch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1,... ## $ Ticket &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;1138... ## $ Fare &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, ... ## $ Cabin &lt;chr&gt; NA, &quot;C85&quot;, NA, &quot;C123&quot;, NA, NA, &quot;E46&quot;, NA, NA, NA, ... ## $ Embarked &lt;chr&gt; &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;C&quot;, ... 7.2 Tidy the data One can already see that we should put Survived, Sex and Embarked as factor. mydata$Survived &lt;- factor(mydata$Survived) mydata$Sex &lt;- factor(mydata$Sex) mydata$Embarked &lt;- factor(mydata$Embarked) 7.3 Understand the data This step consists in massaging our variables to see if we can construct new ones or create additional meaning from what we have. This step require some additional knowledge related to the data and getting familiar with the topics at hand. 7.3.1 A. Transform the data The great thing about this data set is all the features engineering one can do to increase the predictibilty power of our model. 7.3.1.1 Dealing with names. One of the thing one can notice is the title associated with the name. The full names on their own might have little predictibility power, but the title in the name might have some value and can be used as an additional variables. glimpse(mydata$Name) ## chr [1:1309] &quot;Braund, Mr. Owen Harris&quot; ... ## gsub is never fun to use. But we need to strip the cell up to the comma, ## then everything after the point of the title. mydata$title &lt;- gsub(&#39;(.*,)|(\\\\..*)&#39;, &quot;&quot;, mydata$Name) table(mydata$Sex,mydata$title) ## ## Capt Col Don Dona Dr Jonkheer Lady Major Master Miss ## female 0 0 0 1 1 0 1 0 0 260 ## male 1 4 1 0 7 1 0 2 61 0 ## ## Mlle Mme Mr Mrs Ms Rev Sir the Countess ## female 2 1 0 197 2 0 0 1 ## male 0 0 757 0 0 8 1 0 Some titles are just translations from other languages. Let’s regroup those. Some other titles aren’t occuring often and would not justify to have a category on their own. We have regroup some titles under common category. There is some arbitraire in here. mydata$title &lt;- gsub(&quot;Mlle&quot;, &quot;Miss&quot;, mydata$title) mydata$title &lt;- gsub(&quot;Mme&quot;, &quot;Mrs&quot;, mydata$title) mydata$title &lt;- gsub(&quot;Ms&quot;, &quot;Miss&quot;, mydata$title) mydata$title &lt;- gsub(&quot;Jonkheer&quot;, &quot;Mr&quot;, mydata$title) mydata$title &lt;- gsub(&quot;Capt|Col|Major&quot;, &quot;Army&quot;, mydata$title) mydata$title &lt;- gsub(&quot;Don|Dona|Lady|Sir|the Countess&quot;, &quot;Nobility&quot;, mydata$title) mydata$title &lt;- gsub(&quot;Dr|Rev&quot;, &quot;Others&quot;, mydata$title) mydata$title &lt;- factor(mydata$title) mydata$title &lt;- factor(mydata$title, levels(mydata$title)[c(5, 3, 2, 4, 7, 1, 6)] ) table(mydata$Sex, mydata$title) ## ## Mrs Miss Master Mr Others Army Nobility ## female 198 264 0 0 1 0 3 ## male 0 0 61 758 15 7 2 It would be also interesting in fact to check the proportion of survivors for each type of title. round(prop.table(table(mydata$Survived, mydata$title), 2), 2) ## ## Mrs Miss Master Mr Others Army Nobility ## 0 0.21 0.30 0.42 0.84 0.77 0.60 0.25 ## 1 0.79 0.70 0.57 0.16 0.23 0.40 0.75 We can notice that Mrs are more likely to survive than Miss. As expected, our Mr have a very low likelyhood of success. Our Noble title managed mostly to survive. Our next step is to create a Last_Name variable. This could be helpful as the ways family have escaped the boat might hold some pattens. ## To get the last name we strip everything after the first comma. mydata$last_name &lt;- gsub(&quot;,.*&quot;, &quot;&quot;, mydata$Name) ## We can now put this as factor and check how many families. mydata$last_name &lt;- factor(mydata$last_name) So we have 875 different families on board of the Titanic. Of course, there might have different families with the same last name. If that’s the case, we won’t know. 7.3.2 A. Vizualize with families. We could add a variable about the family size. mydata$family_size &lt;- mydata$SibSp + mydata$Parch + 1 If we plot that to check survivalship in function of family size, one can notice interesting patterns. x &lt;- mydata[1:891,] ggplot(x, aes(x = family_size, fill = factor(Survived))) + geom_bar(stat = &#39;count&#39;, position = &quot;dodge&quot;) + scale_x_continuous(breaks = c(1:11)) + labs(x = &quot;Family Size&quot;, fill = &quot;Survived&quot;, title = &quot;Survivalship by Family Size&quot;) + theme(legend.position = c(0.9, 0.8), panel.background = NULL) Obviously, we only have the survivalship for the train set of data, as we have to guess the test set of data. So from what we have, there is a clear advantage in being a family of 2, 3 or 4. We could collapse the variable Family_Size into 3 levels. mydata$family_size_type[mydata$family_size == 1] &lt;- &quot;Singleton&quot; mydata$family_size_type[mydata$family_size &lt;= 4 &amp; mydata$family_size &gt; 1] &lt;- &quot;Small&quot; mydata$family_size_type[mydata$family_size &gt; 4] &lt;- &quot;Large&quot; mydata$family_size_type &lt;- factor(mydata$family_size_type, levels = c(&quot;Singleton&quot;, &quot;Small&quot;, &quot;Large&quot;)) We can see how many people in each category, then we plot the proportion of survivers in each category. x &lt;- mydata[1:891,] table(x$Survived, x$family_size_type) ## ## Singleton Small Large ## 0 374 123 52 ## 1 163 169 10 library(ggmosaic) ggplot(data = x) + geom_mosaic(aes(x = product(family_size_type), fill = Survived)) + labs(x = &quot;Family Size&quot;, y = &quot;Proportion&quot;) + theme(panel.background = NULL) Clearly, there is an advantage in being in a family of size 2, 3 or 4; while there is a disadventage in being part of of a bigger family. We can try to digg in a bit further with our new family size and titles. For people who are part of a Small family size, which title are more likely to surived? y &lt;- x %&gt;% dplyr::filter(family_size_type == &quot;Small&quot;) table(y$Survived, y$title) ## ## Mrs Miss Master Mr Others Army Nobility ## 0 17 13 0 89 3 1 0 ## 1 78 46 22 20 1 0 2 ggplot(data = y) + geom_mosaic(aes(x = product(title), fill = Survived)) + labs(x = &quot;Survivorship for Small Families in function of their title&quot;, y = &quot;Proportion&quot;) + theme(panel.background = NULL, axis.text.x = element_text(angle=90, vjust=1)) All masters in small families have survived. Miss &amp; Mrs in small family size have also lots of chane of survival. Similarly, for people who embarked alone (Singleton), which title are more likely to surived? y &lt;- x %&gt;% filter(family_size_type == &quot;Singleton&quot;) table(y$Survived, y$title) ## ## Mrs Miss Master Mr Others Army Nobility ## 0 2 25 0 337 7 2 1 ## 1 19 78 0 61 2 2 1 ggplot(data = y) + geom_mosaic(aes(x = product(title), fill = Survived)) + labs(x = &quot;Survivorship for people who boarded alone in function of their title&quot;, y = &quot;Proportion&quot;) + theme(panel.background = NULL, axis.text.x = element_text(angle=90, vjust=1)) It might not comes as clear, but we could do the same for title and gender. Vertically the stacks are ordered as Singleton then Small then Large. ggplot(data = x) + geom_mosaic(aes(x = product(family_size_type, title), fill = Survived)) + labs(x = &quot;Survivorship in function of family type and title summary&quot;, y = &quot;Proportion&quot;) + theme(panel.background = NULL, axis.text.x = element_text(angle=90, vjust=1)) 7.4 A. Visualize with cabins. Although there are many missing data there, we can use the cabin number given to passengers. The first letter of the cabin number correspond to the deck on the boat. So let’s strip that deck location from the cabin number. x$deck &lt;- gsub(&quot;([A-Z]+).*&quot;, &quot;\\\\1&quot;, x$Cabin) y &lt;- x %&gt;% filter(!is.na(deck)) table(x$Survived, x$deck) ## ## A B C D E F G T ## 0 8 12 24 8 8 5 2 1 ## 1 7 35 35 25 24 8 2 0 ggplot(data = y) + geom_mosaic(aes(x = product(deck), fill = Survived)) + labs(x = &quot;Survivorship in function of Deck Location&quot;, y = &quot;Proportion&quot;) + theme(panel.background = NULL, axis.text.x = element_text(angle=90, vjust=1)) detach(&quot;package:ggmosaic&quot;, unload=TRUE) There is a bit of an anomaly here as it almost as if most people survived. Now let’s keep in mind, that this is only for people which we have their cabin data. Let’s have a look at how the Passenger Class are distributed on the decks. As we are also finishing this first round of feature engineering, let’s just mention also how the Passenger Class is affecting survivalship. table(x$Pclass, x$deck) ## ## A B C D E F G T ## 1 15 47 59 29 25 0 0 1 ## 2 0 0 0 4 4 8 0 0 ## 3 0 0 0 0 3 5 4 0 round(prop.table(table(x$Survived, x$Pclass), 2), 2) ## ## 1 2 3 ## 0 0.37 0.53 0.76 ## 1 0.63 0.47 0.24 More first class people have survived than other classes. 7.5 B. Transform Dealing with missing data. 7.5.1 Overview. I found this very cool package called visdat based on ggplot2 that help us visualize easily missing data. visdat::vis_dat(mydata) Straight away one can see that the variables cabin and and Age have quite a lot of missing data. For more accuracy one could check fun1 &lt;- function(x){sum(is.na(x))} map_dbl(mydata, fun1) ## PassengerId Survived Pclass Name ## 0 418 0 0 ## Sex Age SibSp Parch ## 0 263 0 0 ## Ticket Fare Cabin Embarked ## 0 1 1014 2 ## title last_name family_size family_size_type ## 0 0 0 0 So we can see some missing data in Fare and in Embarked as well. Let’s deal with these last 2 variables first. 7.5.1.1 Basic Replacement. We first start with the dessert and the variables that have few missing data. For those, one can take the median of similar data. y &lt;- which(is.na(mydata$Embarked)) glimpse(mydata[y, ]) ## Observations: 2 ## Variables: 16 ## $ PassengerId &lt;int&gt; 62, 830 ## $ Survived &lt;fctr&gt; 1, 1 ## $ Pclass &lt;int&gt; 1, 1 ## $ Name &lt;chr&gt; &quot;Icard, Miss. Amelie&quot;, &quot;Stone, Mrs. George Ne... ## $ Sex &lt;fctr&gt; female, female ## $ Age &lt;dbl&gt; 38, 62 ## $ SibSp &lt;int&gt; 0, 0 ## $ Parch &lt;int&gt; 0, 0 ## $ Ticket &lt;chr&gt; &quot;113572&quot;, &quot;113572&quot; ## $ Fare &lt;dbl&gt; 80, 80 ## $ Cabin &lt;chr&gt; &quot;B28&quot;, &quot;B28&quot; ## $ Embarked &lt;fctr&gt; NA, NA ## $ title &lt;fctr&gt; Miss, Mrs ## $ last_name &lt;fctr&gt; Icard, Stone ## $ family_size &lt;dbl&gt; 1, 1 ## $ family_size_type &lt;fctr&gt; Singleton, Singleton So the 2 passengers that have no data on the origin of their embarqument are 2 ladies that boarded alone and that shared the same room in first class and that paid $80. Let’s see who might have paid $80 for a fare. y &lt;- mydata %&gt;% filter(!is.na(Embarked)) ggplot(y, aes(x = Embarked, y = Fare, fill = factor(Pclass))) + geom_boxplot() + scale_y_continuous(labels = scales::dollar, limits = c(0, 250)) + labs(fill = &quot;Passenger \\n Class&quot;) + geom_hline(aes(yintercept = 80), color = &quot;red&quot;, linetype = &quot;dashed&quot;, lwd = 1) + theme(legend.position = c(0.9, 0.8), panel.background = NULL) Following this graph, the 2 passengers without origin of embarcation are most likely from “C”. That said, one can argue that the 2 ladies should have embarked from “S” as this is where most people embarked as shown in this table. table(mydata$Embarked) ## ## C Q S ## 270 123 914 That said, if we filter our data for the demographics of these 2 ladies, the likelhood of coming from “S” decreased quite a bit. x &lt;- mydata %&gt;% filter(Sex == &quot;female&quot;, Pclass == 1, family_size == 1) table(x$Embarked) ## ## C Q S ## 30 0 20 So if we go with median price and with the demographics of the ladies, it would be more likely that they come from “C”. So let’s input that. mydata$Embarked[c(62, 830)] &lt;- &quot;C&quot; Now onto that missing Fare data y &lt;- which(is.na(mydata$Fare)) glimpse(mydata[y, ]) ## Observations: 1 ## Variables: 16 ## $ PassengerId &lt;int&gt; 1044 ## $ Survived &lt;fctr&gt; NA ## $ Pclass &lt;int&gt; 3 ## $ Name &lt;chr&gt; &quot;Storey, Mr. Thomas&quot; ## $ Sex &lt;fctr&gt; male ## $ Age &lt;dbl&gt; 60.5 ## $ SibSp &lt;int&gt; 0 ## $ Parch &lt;int&gt; 0 ## $ Ticket &lt;chr&gt; &quot;3701&quot; ## $ Fare &lt;dbl&gt; NA ## $ Cabin &lt;chr&gt; NA ## $ Embarked &lt;fctr&gt; S ## $ title &lt;fctr&gt; Mr ## $ last_name &lt;fctr&gt; Storey ## $ family_size &lt;dbl&gt; 1 ## $ family_size_type &lt;fctr&gt; Singleton That passenger is a male that boarded in Southampton in third class. So let’s take the median price for similar passagers. y &lt;- mydata %&gt;% filter(Embarked == &quot;S&quot; &amp; Pclass == &quot;3&quot; &amp; Sex == &quot;male&quot; &amp; family_size == 1 &amp; Age &gt; 40) median(y$Fare, na.rm = TRUE) ## [1] 7.8521 mydata$Fare[1044] &lt;- median(y$Fare, na.rm = TRUE) 7.5.1.2 Predictive modeling replacement. First, we’ll focus on the Age variable. There are several methods to input missing data. We’ll try 2 different ones in here. But before we can go forward, we have to factorise some variables. Let’s do the same with Sibsp and Parch mydata$Pclass &lt;- factor(mydata$Pclass) The first method we’ll be using is with the missForest package. y &lt;- mydata %&gt;% select(Pclass, Sex, Fare, Embarked, title, family_size, SibSp, Parch, Age) y &lt;- data.frame(y) library(missForest) z1 &lt;- missForest(y, maxiter = 50, ntree = 500) z1 &lt;- z1[[1]] # To view the new ages # View(z1[[1]]) detach(&quot;package:missForest&quot;, unload=TRUE) The process is fairly rapid on my computer (around 10~15 seconds) Our second method takes slightly more time. This time we are using the mice package. y &lt;- mydata %&gt;% select(Pclass, Sex, Fare, Embarked, title, family_size, SibSp, Parch, Age) y$Pclass &lt;- factor(y$Pclass) y$family_size &lt;- factor(y$family_size) y &lt;- data.frame(y) library(mice) mice_mod &lt;- mice(y, method = &#39;rf&#39;) z2 &lt;- complete(mice_mod) # To view the new ages #View(z2[[1]]) detach(&quot;package:mice&quot;, unload=TRUE) let’s compare both type of imputations. p1 &lt;- ggplot(mydata, aes(x = mydata$Age)) + geom_histogram(aes(y = ..density.., fill = ..count..),binwidth = 5) + labs(x = &quot;Age&quot;, y = &quot;Frequency&quot;, fil = &quot;Survived&quot;) + theme(legend.position = &quot;none&quot;) p2 &lt;- ggplot(z1, aes(x = z1$Age)) + geom_histogram(aes(y = ..density.., fill = ..count..),binwidth = 5) + labs(x = &quot;Age&quot;, y = &quot;Frequency&quot;, fil = &quot;Survived&quot;) + theme(legend.position = &quot;none&quot;) p3 &lt;- ggplot(z2, aes(x = z2$Age)) + geom_histogram(aes(y = ..density.., fill = ..count..),binwidth = 5) + labs(x = &quot;Age&quot;, y = &quot;Frequency&quot;, fil = &quot;Survived&quot;) + theme(legend.position = &quot;none&quot;) multiplot(p1, p2, p3, cols = 3) It does seem like our second method for imputation follow better our first graph. So let’s use that one and input our predicted age into our main dataframe. mydata$Age &lt;- z2$Age 7.5.2 C. Transform More feature engineering with the ages and others. Now that we have filled the NA for the age variable. we can massage a bit more that variable. We can create 3 more variables: Infant from 0 to 5 years old. Child from 5 to 15 years old. Mothers if it is a woman with the variable Parch which is greater than one. mydata$infant &lt;- factor(if_else(mydata$Age &lt;= 5, 1, 0)) mydata$child &lt;- factor(if_else((mydata$Age &gt; 5 &amp; mydata$Age &lt; 15), 1, 0)) mydata$mother &lt;- factor(if_else((mydata$Sex == &quot;female&quot; &amp; mydata$Parch != 0), 1, 0)) mydata$single &lt;- factor(if_else((mydata$SibSp + mydata$Parch + 1 == 1), 1, 0)) 7.6 References. Exploring the titanic dataset from Megan Risdal. here The visdat package. here The ggmosaic package. here "],
["final-words.html", "Chapter 8 Final Words", " Chapter 8 Final Words We have finished a nice book. "],
["references-1.html", "References", " References "]
]
